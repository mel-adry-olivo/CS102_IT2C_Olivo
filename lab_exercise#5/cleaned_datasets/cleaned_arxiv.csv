"","title","author","subject","abstract","date"
"1","learn to code sustainably: an empirical study on llm-based green code generation","tina vartziotis, ippolyti dellatolas, george dasoulas, maximilian schmidt, florian schneider, tim hoffmann, sotirios kotsopoulos, michael keckeisen","software engineering","the increasing use of information technology has led to a significant share of energy consumption and carbon emissions from data centers. these contributions are expected to rise with the growing demand for big data analytics, increasing digitization, and the development of large artificial intelligence (ai) models. the need to address the environmental impact of software development has led to increased interest in green (sustainable) coding and claims that the use of ai models can lead to energy efficiency gains. here, we provide an empirical study on green code and an overview of green coding practices, as well as metrics used to quantify the sustainability awareness of ai models. in this framework, we evaluate the sustainability of auto-generated code. the auto-generate codes considered in this study are produced by generative commercial ai language models, github copilot, openai chatgpt-3, and amazon codewhisperer. within our methodology, in order to quantify the sustainability awareness of these ai models, we propose a definition of the code's ""green capacity"", based on certain sustainability metrics. we compare the performance and green capacity of human-generated code and code generated by the three ai language models in response to easy-to-hard problem statements. our findings shed light on the current capacity of ai models to contribute to sustainable software development.",2024-03-05
"2","alloyinecore: embedding of first-order relational logic into meta-object facility for automated model reasoning","ferhat erata, arda goknil, ivan kurtev, bedir tekinerdogan","software engineering","we present alloyinecore, a tool for specifying metamodels with their static semantics to facilitate automated, formal reasoning on models. software development projects require that software systems be specified in various models (e.g., requirements models, architecture models, test models, and source code). it is crucial to reason about those models to ensure the correct and complete system specifications. alloyinecore allows the user to specify metamodels with their static semantics, while, using the semantics, it automatically detects inconsistent models, and completes partial models. it has been evaluated on three industrial case studies in the automotive domain (this https url).",2024-03-05
"3","can llms generate architectural design decisions? -an exploratory empirical study","rudra dhar, karthik vaidhyanathan, vasudeva varma","software engineering","architectural knowledge management (akm) involves the organized handling of information related to architectural decisions and design within a project or organization. an essential artifact of akm is the architecture decision records (adr), which documents key design decisions. adrs are documents that capture decision context, decision made and various aspects related to a design decision, thereby promoting transparency, collaboration, and understanding. despite their benefits, adr adoption in software development has been slow due to challenges like time constraints and inconsistent uptake. recent advancements in large language models (llms) may help bridge this adoption gap by facilitating adr generation. however, the effectiveness of llm for adr generation or understanding is something that has not been explored. to this end, in this work, we perform an exploratory study that aims to investigate the feasibility of using llm for the generation of adrs given the decision context. in our exploratory study, we utilize gpt and t5-based models with 0-shot, few-shot, and fine-tuning approaches to generate the decision of an adr given its context. our results indicate that in a 0-shot setting, state-of-the-art models such as gpt-4 generate relevant and accurate design decisions, although they fall short of human-level performance. additionally, we observe that more cost-effective models like gpt-3.5 can achieve similar outcomes in a few-shot setting, and smaller models such as flan-t5 can yield comparable results after fine-tuning. to conclude, this exploratory study suggests that llm can generate design decisions, but further research is required to attain human-level generation and establish standardized widespread adoption.",2024-03-04
"4","a systematic evaluation of large language models for generating programming code","wenpin hou, zhicheng ji","software engineering","we systematically evaluated the performance of seven large language models in generating programming code using various prompt strategies, programming languages, and task difficulties. gpt-4 substantially outperforms other large language models, including gemini ultra and claude 2. the coding performance of gpt-4 varies considerably with different prompt strategies. in most leetcode and geeksforgeeks coding contests evaluated in this study, gpt-4 employing the optimal prompt strategy outperforms 85 percent of human participants. additionally, gpt-4 demonstrates strong capabilities in translating code between different programming languages and in learning from past errors. the computational efficiency of the code generated by gpt-4 is comparable to that of human programmers. these results suggest that gpt-4 has the potential to serve as a reliable assistant in programming code generation and software development.",2024-03-01
"5","reusable mlops: reusable deployment, reusable infrastructure and hot-swappable machine learning models and services","d panchal, p verma, i baran, d musgrove, d lu","software engineering","although machine learning model building has become increasingly accessible due to a plethora of tools, libraries and algorithms being available freely, easy operationalization of these models is still a problem. it requires considerable expertise in data engineering, software development, cloud and devops. it also requires planning, agreement, and vision of how the model is going to be used by the business applications once it is in production, how it is going to be continuously trained on fresh incoming data, and how and when a newer model would replace an existing model. this leads to developers and data scientists working in silos and making suboptimal decisions. it also leads to wasted time and effort. we introduce the acumos ai platform we developed and we demonstrate some unique novel capabilities that the acumos model runner possesses, that can help solve the above problems. we introduce a new sustainable concept in the field of ai/ml operations - called reusable mlops - where we reuse the existing deployment and infrastructure to serve new models by hot-swapping them without tearing down the infrastructure or the microservice, thus achieving reusable deployment and operations for ai/ml models while still having continuously trained models in production.",2024-02-19
"6","are your comments outdated? towards automatically detecting code-comment consistency","yuan huang, yinan chen, xiangping chen, xiaocong zhou","software engineering","in software development and maintenance, code comments can help developers understand source code, and improve communication among developers. however, developers sometimes neglect to update the corresponding comment when changing the code, resulting in outdated comments (i.e., inconsistent codes and comments). outdated comments are dangerous and harmful and may mislead subsequent developers. more seriously, the outdated comments may lead to a fatal flaw sometime in the future. to automatically identify the outdated comments in source code, we proposed a learning-based method, called cocc, to detect the consistency between code and comment. to efficiently identify outdated comments, we extract multiple features from both codes and comments before and after they change. besides, we also consider the relation between code and comment in our model. experiment results show that cocc can effectively detect outdated comments with precision over 90%. in addition, we have identified the 15 most important factors that cause outdated comments, and verified the applicability of cocc in different programming languages. we also used cocc to find outdated comments in the latest commits of open source projects, which further proves the effectiveness of the proposed method.",2024-03-01
"7","a naive approach for automatic line-level code completion","shamima naznin, dr.manishankar mondal","software engineering","coding is an integral aspect of programming. a programmer can automatically complete a code fragment after writing a few tokens, and the process of automatic completion is known as code completion. several research studies on code completion have previously been conducted for method body completion and method parameter completion. however, this fundamental study explores the automatic completion of any program statement that might not even be part of a method.
the goal is to provide suggestions to the programmer for completing code throughout the codebase by identifying and analyzing code similarities. the proposed methodology can be regarded as a fundamental framework for automated code completion. from the investigation of hundreds of revisions of four subject systems written in c and java, it is observed that the proposed method can automatically complete around 22% of code statements with an average accuracy of 87% that a programmer writes during development, accelerating software development time. the empirical analysis further demonstrates that the approach can be used with programming language neutrality.
the study concludes by illustrating that taking 10 characters as prefixes before invoking completion provides maximum precision.",2024-02-29
"8","devphish: exploring social engineering in software supply chain attacks on developers","hossein siadati, sima jafarikhah, elif sahin, terrence brent hernandez, elijah lorenzo tripp, denis khryashchev","software engineering","the software supply chain (ssc) has captured considerable attention from attackers seeking to infiltrate systems and undermine organizations. there is evidence indicating that adversaries utilize social engineering (soce) techniques specifically aimed at software developers. that is, they interact with developers at critical steps in the software development life cycle (sdlc), such as accessing github repositories, incorporating code dependencies, and obtaining approval for pull requests (pr) to introduce malicious code. this paper aims to comprehensively explore the existing and emerging soce tactics employed by adversaries to trick software engineers (swes) into delivering malicious software. by analyzing a diverse range of resources, which encompass established academic literature and real-world incidents, the paper systematically presents an overview of these manipulative strategies within the realm of the ssc. such insights prove highly beneficial for threat modeling and security gap analysis.",2024-02-28
"9","low-modeling of software systems","jordi cabot","software engineering","there is a growing need for better development methods and tools to keep up with the increasing complexity of new software systems. new types of user interfaces, the need for intelligent components, sustainability concerns, ... bring new challenges that we need to handle. in the last years, model-driven engineering has been key to improving the quality and productivity of software development, but models themselves are becoming increasingly complex to specify and manage. in this paper, we present the concept of low-modeling as a solution to enhance current model-driven engineering techniques and get them ready for this new generation of software systems.",2024-02-28
"10","autonomous vehicles: evolution of artificial intelligence and learning algorithms","divya garikapati, sneha sudhir shetiya","machine learning","the advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies. central to this evolution is the integration of artificial intelligence (ai) and learning algorithms, propelling vehicles into realms of unprecedented autonomy. this paper provides a comprehensive exploration of the evolutionary trajectory of ai within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements. commencing with a current landscape overview, the paper delves into the fundamental role of ai in shaping the autonomous decision-making capabilities of vehicles. it elucidates the steps involved in the ai-powered development life cycle in vehicles, addressing ethical considerations and bias in ai-driven software development for autonomous vehicles. the study presents statistical insights into the usage and types of ai/learning algorithms over the years, showcasing the evolving research landscape within the automotive industry. furthermore, the paper highlights the pivotal role of parameters in refining algorithms for both trucks and cars, facilitating vehicles to adapt, learn, and improve performance over time. it concludes by outlining different levels of autonomy, elucidating the nuanced usage of ai and learning algorithms, and automating key tasks at each level. additionally, the document discusses the variation in software package sizes across different autonomy levels",2024-02-27
"11","chronicles of ci/cd: a deep dive into its usage over time","hugo da gião, andré flores, rui pereira, jácome cunha","software engineering","devops is a combination of methodologies and tools that improves the software development, build, deployment, and monitoring processes by shortening its lifecycle and improving software quality. part of this process is ci/cd, which embodies mostly the first parts, right up to the deployment. despite the many benefits of devops and ci/cd, it still presents many challenges promoted by the tremendous proliferation of different tools, languages, and syntaxes, which makes the field quite challenging to learn and keep up to date. software repositories contain data regarding various software practices, tools, and uses. this data can help gather multiple insights that inform technical and academic decision-making. github is currently the most popular software hosting platform and provides a search api that lets users query its repositories. our goal with this paper is to gain insights into the technologies developers use for ci/cd by analyzing github repositories. using a list of the state-of-the-art ci/cd technologies, we use the github search api to find repositories using each of these technologies. we also use the api to extract various insights regarding those repositories. we then organize and analyze the data collected. from our analysis, we provide an overview of the use of ci/cd technologies in our days, but also what happened in the last 12 years. we also show developers use several technologies simultaneously in the same project and that the change between technologies is quite common. from these insights, we find several research paths, from how to support the use of multiple technologies, both in terms of techniques, but also in terms of human-computer interaction, to aiding developers in evolving their ci/cd pipelines, again considering the various dimensions of the problem.",2024-02-27
"12","leveraging power of deep learning for fast and efficient elite pixel selection in time series sar interferometry","ashutosh tiwari, nitheshnirmal sadhashivam, leonard o. ohenhen, manoochehr shirzaei","signal processing","this work proposes an improved convolutional long short-term memory (convlstm) based architecture for selection of elite pixels (i.e., less noisy) in time series interferometric synthetic aperture radar (ts-insar). compared to previous version, the model can process insar stacks of variable time steps and select both persistent (ps) and distributed scatterers (ds). we trained the model on ~20,000 training images (interferograms), each of size 100 by 100 pixels, extracted from insar time series interferograms containing both artificial features (buildings and infrastructure) and objects of natural environment (vegetation, forests, barren or agricultural land, water bodies). based on such categorization, we developed two deep learning models, primarily focusing on urban and coastal sites. training labels were generated from elite pixel selection outputs generated from the wavelet-based insar (wabinsar) software developed by shirzaei (2013) and improved in lee and shirzaei (2023). with 4 urban and 7 coastal sites used for training and validation, the predicted elite pixel selection maps reveal that the proposed models efficiently learn from wabinsar-generated labels, reaching a validation accuracy of 94%. the models accurately discard pixels affected by geometric and temporal decorrelation while selecting pixels corresponding to urban objects and those with stable phase history unaffected by temporal and geometric decorrelation. the density of pixels in urban areas is comparable to and higher for coastal areas compared to wabinsar outputs. with significantly reduced time computation (order of minutes) and improved selection of elite pixels, the proposed models can efficiently process long insar time series stacks and generate rapid deformation maps.",2024-02-26
"13","rethinking software engineering in the foundation model era: a curated catalogue of challenges in the development of trustworthy fmware","ahmed e. hassan, dayi lin, gopi krishnan rajbahadur, keheliya gallaba, filipe r. cogo, boyuan chen, haoxiang zhang, kishanthan thangarajah, gustavo ansaldi oliva, jiahuei lin, wali mohammad abdullah, zhen ming jiang","software engineering","foundation models (fms), such as large language models (llms), have revolutionized software development by enabling new use cases and business models. we refer to software built using fms as fmware. the unique properties of fmware (e.g., prompts, agents, and the need for orchestration), coupled with the intrinsic limitations of fms (e.g., hallucination) lead to a completely new set of software engineering challenges. based on our industrial experience, we identified 10 key se4fmware challenges that have caused enterprise fmware development to be unproductive, costly, and risky. in this paper, we discuss these challenges in detail and state the path for innovation that we envision. next, we present fmarts, which is our long-term effort towards creating a cradle-to-grave platform for the engineering of trustworthy fmware. finally, we (i) show how the unique properties of fmarts enabled us to design and develop a complex fmware for a large customer in a timely manner and (ii) discuss the lessons that we learned in doing so. we hope that the disclosure of the aforementioned challenges and our associated efforts to tackle them will not only raise awareness but also promote deeper and further discussions, knowledge sharing, and innovative solutions across the software engineering discipline.",2024-02-25
"14","chain-of-specificity: an iteratively refining method for eliciting knowledge from large language models","kaiwen wei, jingyuan zhang, hongzhi zhang, fuzheng zhang, di zhang, li jin, yue yu","artificial intelligence","large language models (llms) exhibit remarkable generative capabilities, enabling the generation of valuable information. despite these advancements, previous research found that llms sometimes struggle with adhering to specific constraints (e.g., in specific place or at specific time), at times even overlooking them, which leads to responses that are either too generic or not fully satisfactory. existing approaches attempted to address this issue by decomposing or rewriting input instructions, yet they fall short in adequately emphasizing specific constraints and in unlocking the underlying knowledge (e.g., programming within the context of software development). in response, this paper proposes a simple yet effective method named chain-of-specificity (cos). specifically, cos iteratively emphasizes the specific constraints in the input instructions, unlocks knowledge within llms, and refines responses. experiments conducted on publicly available and self-build complex datasets demonstrate that cos outperforms existing methods in enhancing generated content especially for the specificity. besides, as the number of specific constraints increase, other baselines falter, while cos still performs well. moreover, we show that distilling responses generated by cos effectively enhances the ability of smaller models to follow the constrained instructions. resources of this paper will be released for further research.",2024-02-20
"15","studying llm performance on closed- and open-source data","toufique ahmed, christian bird, premkumar devanbu, saikat chakraborty","software engineering","large language models (llms) are finding wide use in software engineering practice. these models are extremely data-hungry, and are largely trained on open-source (oss) code distributed with permissive licenses. in terms of actual use however, a great deal of software development still occurs in the for-profit/proprietary sphere, where the code under development is not, and never has been, in the public domain; thus, many developers, do their work, and use llms, in settings where the models may not be as familiar with the code under development. in such settings, do llms work as well as they do for oss code? if not, what are the differences? when performance differs, what are the possible causes, and are there work-arounds? in this paper, we examine this issue using proprietary, closed-source software data from microsoft, where most proprietary code is in c# and c++. we find that performance for c# changes little from oss --> proprietary code, but does significantly reduce for c++; we find that this difference is attributable to differences in identifiers. we also find that some performance degradation, in some cases, can be ameliorated efficiently by in-context learning.",2024-02-23
"16","llm-compdroid: repairing configuration compatibility bugs in android apps with pre-trained large language models","zhijie liu, yutian tang, meiyun li, xin jin, yunfei long, liang feng zhang, xiapu luo","software engineering","xml configurations are integral to the android development framework, particularly in the realm of ui display. however, these configurations can introduce compatibility issues (bugs), resulting in divergent visual outcomes and system crashes across various android api versions (levels). in this study, we systematically investigate llm-based approaches for detecting and repairing configuration compatibility bugs. our findings highlight certain limitations of llms in effectively identifying and resolving these bugs, while also revealing their potential in addressing complex, hard-to-repair issues that traditional tools struggle with. leveraging these insights, we introduce the llm-compdroid framework, which combines the strengths of llms and traditional tools for bug resolution. our experimental results demonstrate a significant enhancement in bug resolution performance by llm-compdroid, with llm-compdroid-gpt-3.5 and llm-compdroid-gpt-4 surpassing the state-of-the-art tool, conffix, by at least 9.8% and 10.4% in both correct and correct@k metrics, respectively. this innovative approach holds promise for advancing the reliability and robustness of android applications, making a valuable contribution to the field of software development.",2024-02-23
"17","agile requirement change management model for global software development","neha koulecar, bachan ghimire","software engineering","we propose a noble, comprehensive and robust agile requirements change management (arcm) model that addresses the limitations of existing models and is tailored for agile software development in the global software development paradigm. to achieve this goal, we conducted an exhaustive literature review and an empirical study with rcm industry experts. our study evaluated the effectiveness of the proposed rcm model in a real-world setting and identifies any limitations or areas for improvement. the results of our study provide valuable insights into how the proposed rcm model can be applied in agile global software development environments to improve software development practices and optimize project success rates.",2024-02-22
"18","bugfix: towards a common language and framework for the automaticprogram repair community","bertrand meyer, viktoryia kananchuk, li huang","software engineering","techniques of automatic program repair (apr) have the potential of thoroughly facilitating the task of producing quality software. after a promising start, however, progress in making apr practical has been hindered by the lack of a common framework to support the multiplicity of apr ideas and tools, and of target programming languages and environments.
in this position paper we outline a general framework to enable the apr community to benefit from each otherś advances, in particular through a standard language for describing bugs and their fixes. such a common framework (which is also applicable to work on fault seeding) could be a tremendous benefit to researchers and developers of interactive development environments (ides) who are working to make apr an effective part of the practical experience of software developers.",2024-02-22
"19","copilot evaluation harness: evaluating llm-guided software programming","anisha agarwal, aaron chan, shubham chandel, jinu jang, shaun miller, roshanak zilouchian moghaddam, yevhen mohylevskyy, neel sundaresan, michele tufano","software engineering","the integration of large language models (llms) into development environments (ides) has become a focal point in modern software development. llms such as openai gpt-3.5/4 and code llama offer the potential to significantly augment developer productivity by serving as intelligent, chat-driven programming assistants. however, utilizing llms out of the box is unlikely to be optimal for any given scenario. rather, each system requires the llm to be honed to its set of heuristics to ensure the best performance. in this paper, we introduce the copilot evaluation harness: a set of data and tools for evaluating llm-guided ide interactions, covering various programming scenarios and languages. we propose our metrics as a more robust and information-dense evaluation than previous state of the art evaluation systems. we design and compute both static and execution based success metrics for scenarios encompassing a wide range of developer tasks, including code generation from natural language (generate), documentation generation from code (doc), test case generation (test), bug-fixing (fix), and workspace understanding and query resolution (workspace). these success metrics are designed to evaluate the performance of llms within a given ide and its respective parameter space. our learnings from evaluating three common llms using these metrics can inform the development and validation of future scenarios in llm guided ides.",2024-02-22
"20","test-driven development for code generation","noble saji mathews, meiyappan nagappan","software engineering","large language models (llms) like gpt4, have shown proficiency in generating code snippets from problem statements. traditionally software development by humans followed a similar methodology of writing code from problem statements or requirements. however, in the past, there have been several studies that have shown the value of test-driven development (tdd) where humans write tests based on problem statements before the code for the functionality is written. in the context of llm-based code generation, one obvious benefit of tdd is that the developer then knows for sure if the generated code has passed all the given tests or not. therefore, in this paper, we want to empirically evaluate the hypothesis: giving the problem statements and tests as input to gpt4 is better than just giving the problem statement as input. to test our hypothesis, we build a framework tgen. in our experiments on the mbpp, humaneval and codechef datasets, we consistently find that including tests solves more programming problems than not including them. thus we show that tdd is a better development model than just using a problem statement when using gpt4 for code generation tasks.",2024-02-21
"21","a strategic model of software dependency networks","cornelius fritz, co-pierre georg, angelo mele, michael schweinberger","econometrics","modern software development involves collaborative efforts and reuse of existing code, which reduces the cost of developing new software. however, reusing code from existing packages exposes coders to vulnerabilities in these dependencies. we study the formation of dependency networks among software packages and libraries, guided by a structural model of network formation with observable and unobservable heterogeneity. we estimate costs, benefits, and link externalities of the network of 696,790 directed dependencies between 35,473 repositories of the rust programming language using a novel scalable algorithm. we find evidence of a positive externality exerted on other coders when coders create dependencies. furthermore, we show that coders are likely to link to more popular packages of the same software type but less popular packages of other types. we adopt models for the spread of infectious diseases to measure a package's systemicness as the number of downstream packages a vulnerability would affect. systemicness is highly skewed with the most systemic repository affecting almost 90% of all repositories only two steps away. lastly, we show that protecting only the ten most important repositories reduces vulnerability contagion by nearly 40%.",2024-02-20
"22","a disruptive research playbook for studying disruptive innovations","margaret-anne storey, daniel russo, nicole novielli, takashi kobayashi, dong wang","software engineering","as researchers, we are now witnessing a fundamental change in our technologically-enabled world due to the advent and diffusion of highly disruptive technologies such as generative ai, augmented reality (ar) and virtual reality (vr). in particular, software engineering has been profoundly affected by the transformative power of disruptive innovations for decades, with a significant impact of technical advancements on social dynamics due to its the socio-technical nature. in this paper, we reflect on the importance of formulating and addressing research in software engineering through a socio-technical lens, thus ensuring a holistic understanding of the complex phenomena in this field. we propose a research playbook with the goal of providing a guide to formulate compelling and socially relevant research questions and to identify the appropriate research strategies for empirical investigations, with an eye on the long-term implications of technologies or their use. we showcase how to apply the research playbook. firstly, we show how it can be used retrospectively to reflect on a prior disruptive technology, stack overflow, and its impact on software development. secondly, we show it can be used to question the impact of two current disruptive technologies: ai and ar/vr. finally, we introduce a specialized gpt model to support the researcher in framing future investigations. we conclude by discussing the broader implications of adopting the playbook for both researchers and practitioners in software engineering and beyond.",2024-02-20
"23","choosing a suitable requirement prioritization method: a survey","esraa alhenawi, shatha awawdeh, ruba abu khurma, maribel garcía-arenas, pedro a. castillo, amjad hudaib","software engineering","software requirements prioritization plays a crucial role in software development. it can be viewed as the process of ordering requirements by determining which requirements must be done first and which can be done later. powerful requirements prioritization techniques are of paramount importance to finish the implementation on time and within budget. many factors affect requirement prioritization such as stakeholder expectations, complexity, dependency, scalability, risk, and cost. therefore, finding the proper order of requirements is a challenging process. hence, different types of requirements prioritization techniques have been developed to support this task. in this survey, we propose a novel classification that can classify the prioritization techniques under two major classes: relative and exact prioritization techniques class, where each class is divided into two subclasses. we depend in our classification on the way the value of ranking is given to the requirement, either explicitly as a specific value in the case of the exact prioritization techniques class, or implicitly in the case of the relative prioritization technique class. an overview of fifteen different requirements prioritization techniques are presented and organized according to the proposed classification criteria's. moreover, we make a comparison between methods that are related to the same subclass to analyze their strengths and weaknesses. based on the comparison results, the properties for each proposed subclass of techniques are identified. depending on these properties, we present some recommendations to help project managers in the process of selecting the most suitable technique to prioritize requirements based on their project characteristics (number of requirements, time, cost, and accuracy).",2024-02-20
"24","systematic mapping protocol -- ux design role in software development process","emilio ormeño, fernando pinciroli","software engineering","a systematic mapping protocol is a method for conducting a literature review in a rigorous and transparent way. it aims to provide an overview of the current state of research on a specific topic, identify gaps and opportunities, and guide future work. in this document, we present a systematic mapping protocol for investigating the role of the ux designer in the software development process. we define the research questions, scope, sources, search strategy, selection criteria, data extraction, and analysis methods that we will use to conduct the mapping study. our goal is to understand how the ux designers collaborate with other stakeholders, what methods and tools they use, what challenges they face, and what outcomes they achieve in different contexts and domains.",2024-02-20
"25","inferring non-failure conditions for declarative programs","michael hanus","programming languages","unintended failures during a computation are painful but frequent during software development. failures due to external reasons (e.g., missing files, no permissions) can be caught by exception handlers. programming failures, such as calling a partially defined operation with unintended arguments, are often not caught due to the assumption that the software is correct. this paper presents an approach to verify such assumptions. for this purpose, non-failure conditions for operations are inferred and then checked in all uses of partially defined operations. in the positive case, the absence of such failures is ensured. in the negative case, the programmer could adapt the program to handle possibly failing situations and check the program again. our method is fully automatic and can be applied to larger declarative programs. the results of an implementation for functional logic curry programs are presented.",2024-02-20
"26","measuring impacts of poisoning on model parameters and neuron activations: a case study of poisoning codebert","aftab hussain, md rafiqul islam rabin, navid ayoobi, mohammad amin alipour","software engineering","large language models (llms) have revolutionized software development practices, yet concerns about their safety have arisen, particularly regarding hidden backdoors, aka trojans. backdoor attacks involve the insertion of triggers into training data, allowing attackers to manipulate the behavior of the model maliciously. in this paper, we focus on analyzing the model parameters to detect potential backdoor signals in code models. specifically, we examine attention weights and biases, activation values, and context embeddings of the clean and poisoned codebert models. our results suggest noticeable patterns in activation values and context embeddings of poisoned samples for the poisoned codebert model; however, attention weights and biases do not show any significant differences. this work contributes to ongoing efforts in white-box detection of backdoor signals in llms of code through the analysis of parameters and activations.",2024-02-20
"27","challenges and experiences of iranian developers with mlops at enterprise","mohammad heydari, zahra rezvani","software engineering","data is becoming more complex, and so are the approaches designed to process it. enterprises have access to more data than ever, but many still struggle to glean the full potential of insights from what they have. this research explores the challenges and experiences of iranian developers in implementing the mlops paradigm within enterprise settings. mlops, or machine learning operations, is a discipline focused on automating the continuous delivery of machine learning models. in this study, we review the most popular mlops tools used by leading technology enterprises. additionally, we present the results of a questionnaire answered by over 110 iranian machine learning experts and software developers, shedding light on mlops tools and the primary obstacles faced. the findings reveal that data quality problems, a lack of resources, and difficulties in model deployment are among the primary challenges faced by practitioners. collaboration between ml, devops, ops, and science teams is seen as a pivotal challenge in implementing mlops effectively.",2024-02-19
"28","enhancing large language models for text-to-testcase generation","saranya alagarsamy, chakkrit tantithamthavorn, chetan arora, aldeida aleti","software engineering","context: test-driven development (tdd) is a widely employed software development practice that involves developing test cases based on requirements prior to writing the code. although various methods for automated test case generation have been proposed, they are not specifically tailored for tdd, where requirements instead of code serve as input. objective: in this paper, we introduce a text-to-testcase generation approach based on a large language model (gpt-3.5) that is fine-tuned on our curated dataset with an effective prompt design. method: our approach involves enhancing the capabilities of basic gpt-3.5 for text-to-testcase generation task that is fine-tuned on our curated dataset with an effective prompting design. we evaluated the effectiveness of our approach using a span of five large-scale open-source software projects. results: our approach generated 7k test cases for open source projects, achieving 78.5% syntactic correctness, 67.09% requirement alignment, and 61.7% code coverage, which substantially outperforms all other llms (basic gpt-3.5, bloom, and codet5). in addition, our ablation study demonstrates the substantial performance improvement of the fine-tuning and prompting components of the gpt-3.5 model. conclusions: these findings lead us to conclude that fine-tuning and prompting should be considered in the future when building a language model for the text-to-testcase generation task",2024-02-19
"29","evaluating program repair with semantic-preserving transformations: a naturalness assessment","thanh le-cong, dat nguyen, bach le, toby murray","software engineering","in this paper, we investigate the naturalness of semantic-preserving transformations and their impacts on the evaluation of npr. to achieve this, we conduct a two-stage human study, including (1) interviews with senior software developers to establish the first concrete criteria for assessing the naturalness of code transformations and (2) a survey involving 10 developers to assess the naturalness of 1178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs. our findings reveal that nearly 60% and 20% of these transformations are considered natural and unnatural with substantially high agreement among human annotators. furthermore, the unnatural code transformations introduce a 25.2% false alarm rate on robustness of five well-known npr systems. additionally, the performance of the npr systems drops notably when evaluated using natural transformations, i.e., a drop of up to 22.9% and 23.6% in terms of the numbers of correct and plausible patches generated by these systems. these results highlight the importance of robustness testing by considering naturalness of code transformations, which unveils true effectiveness of npr systems. finally, we conduct an exploration study on automating the assessment of naturalness of code transformations by deriving a new naturalness metric based on cross-entropy. based on our naturalness metric, we can effectively assess naturalness for code transformations automatically with an auc of 0.7.",2024-02-19
"30","can chatgpt support developers? an empirical evaluation of large language models for code generation","kailun jin, chung-yu wang, hung viet pham, hadi hemmati","software engineering","large language models (llms) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. however, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively llms can support developers in real-world. to address this, we conducted an empirical analysis of conversations in devgpt, a dataset collected from developers' conversations with chatgpt (captured with the share link feature on platforms such as github). our empirical findings indicate that the current practice of using llm-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. these findings indicate that there is much future work needed to improve llms in code generation before they can be integral parts of modern software development.",2024-02-18
"31","gradients of brain organization: smooth sailing from methods development to user community","jessica royer, casey paquola, sofie l. valk, matthias kirschner, seok-jun hong, bo-yong park, richard a.i. bethlehem, robert leech, b. t. thomas yeo, elizabeth jefferies, jonathan smallwood, daniel margulies, boris c. bernhardt","other quantitative biology","multimodal neuroimaging grants a powerful in vivo window into the structure and function of the human brain. recent methodological and conceptual advances have enabled investigations of the interplay between large-scale spatial trends, or gradients, in brain structure and function, offering a framework to unify principles of brain organization across multiple scales. strong community enthusiasm for these techniques has been instrumental in their widespread adoption and implementation to answer key questions in neuroscience. following a brief review of current literature on this framework, this perspective paper will highlight how pragmatic steps aiming to make gradient methods more accessible to the community propelled these techniques to the forefront of neuroscientific inquiry. more specifically, we will emphasize how interest for gradient methods was catalyzed by data sharing, open-source software development, as well as the organization of dedicated workshops led by a diverse team of early career researchers. to this end, we argue that the growing excitement for brain gradients is the result of coordinated and consistent efforts to build an inclusive community and can serve as a case in point for future innovations and conceptual advances in neuroinformatics. we close this perspective paper by discussing challenges for the continuous refinement of neuroscientific theory, methodological innovation, and real-world translation to maintain our collective progress towards integrated models of brain organization.",2024-02-16
"32","multidimer: a multi-dimensional bug analyzer","lakmal silva, michael unterkalmsteiner, krzysztof wnuk","software engineering","background: bugs and bug management consumes a significant amount of time and effort from software development organizations. a reduction in bugs can significantly improve the capacity for new feature development. aims: we categorize and visualize dimensions of bug reports to identify accruing technical debt. this evidence can serve practitioners and decision makers not only as an argumentative basis for steering improvement efforts, but also as a starting point for root cause analysis, reducing overall bug inflow. method: we implemented a tool, multidimer, that analyzes and visualizes bug reports. the tool was implemented and evaluated at ericsson. results: we present our preliminary findings using the multidimer for bug analysis, where we successfully identified components generating most of the bugs and bug trends within certain components. conclusions: by analyzing the dimensions provided by multidimer, we show that classifying and visualizing bug reports in different dimensions can stimulate discussions around bug hot spots as well as validating the accuracy of manually entered bug report attributes used in technical debt measurements such as fault slip through.",2024-02-16
"33","language-driven engineering an interdisciplinary software development paradigm","bernhard steffen, tiziana margaria, alexander bainczyk, steve boßelmann, daniel busch, marc driessen, markus frohme, falk howar, sven jörges, marvin krause, marco krumrey, anna-lena lamprecht, michael lybecait, alnis murtovi, stefan naujokat, johannes neubauer, alexander schieweck, jonas schürmann, steven smyth, barbara steffen, fabian storek, tim tegeler, sebastian teumert, dominic wirkner, philip zweihoff","software engineering","we illustrate how purpose-specific, graphical modeling enables application experts with different levels of expertise to collaboratively design and then produce complex applications using their individual, purpose-specific modeling language. our illustration includes seven graphical integrated modeling environments (imes) that support full code generation, as well as four browser-based applications that were modeled and then fully automatically generated and produced using dime, our most complex graphical ime. while the seven imes were chosen to illustrate the types of languages we support with our language-driven engineering (lde) approach, the four dime products were chosen to give an impression of the power of our lde-generated imes. in fact, equinocs, springer nature's future editorial system for proceedings, is also being fully automatically generated and then deployed at their dordrecht site using a deployment pipeline generated with rig, one of the imes presented. our technology is open source and the products presented are currently in use.",2024-02-16
"34","penetration testing and legacy systems","sandra smyth","software engineering","as per adusumilli (2015),'70% of corporate business systems today are legacy applications. recent statistics prove that over 60% of it budget is spent on maintaining these legacy systems, showing the rigidity and the fragile nature of these systems.' usually, testing is included during the software development cycle, using testing techniques such as unit testing, integration testing, and system testing before releasing the product. after the software product is released to production, no additional testing is done; the testing process is back to the table only when modifications are made. techniques such as regression testing are included to ensure the changes do not affect existing functionality, but testing nonfunctional features that are rarely included in such regression tests' scope. schrader (2021) affirms that 'legacy systems are often maintained only to ensure function,' and it organizations may fail to consider the cybersecurity perspective to remain secure. legacy systems are a high-risk component for the organization that must be carefully considered when structuring a cyber security strategy. this paper aims to help the reader understand some measures that can be taken to secure legacy systems, explaining what penetration testing is and how this testing technique can help secure legacy systems. keywords: testing, legacy, security, risks, prevention, mitigation, pentesting.",2023-12-17
"35","reproducing, extending, and analyzing naming experiments","rachel alpern, ido lazer, issar tzachor, hanit hakim, sapir weissbuch, dror g. feitelson","software engineering","naming is very important in software development, as names are often the only vehicle of meaning about what the code is intended to do. a recent study on how developers choose names collected the names given by different developers for the same objects. this enabled a study of these names' diversity and structure, and the construction of a model of how names are created. we reproduce different parts of this study in three independent experiments. importantly, we employ methodological variations rather than striving of an exact replication. when the same results are obtained this then boosts our confidence in their validity by demonstrating that they do not depend on the methodology.
our results indeed corroborate those of the original study in terms of the diversity of names, the low probability of two developers choosing the same name, and the finding that experienced developers tend to use slightly longer names than inexperienced students. we explain name diversity by performing a new analysis of the names, classifying the concepts represented in them as universal (agreed upon), alternative (reflecting divergent views on a topic), or optional (reflecting divergent opinions on whether to include this concept at all). this classification enables new research directions concerning the considerations involved in naming decisions. we also show that explicitly using the model proposed in the original study to guide naming leads to the creation of better names, whereas the simpler approach of just asking participants to use longer and more detailed names does not.",2024-02-15
"36","practitioners' challenges and perceptions of ci build failure predictions at atlassian","yang hong, chakkrit tantithamthavorn, jirat pasuksmit, patanamon thongtanunam, arik friedman, xing zhao, anton krasikov","software engineering","continuous integration (ci) build failures could significantly impact the software development process and teams, such as delaying the release of new features and reducing developers' productivity. in this work, we report on an empirical study that investigates ci build failures throughout product development at atlassian. our quantitative analysis found that the repository dimension is the key factor influencing ci build failures. in addition, our qualitative survey revealed that atlassian developers perceive ci build failures as challenging issues in practice. furthermore, we found that the ci build prediction can not only provide proactive insight into ci build failures but also facilitate the team's decision-making. our study sheds light on the challenges and expectations involved in integrating ci build prediction tools into the bitbucket environment, providing valuable insights for enhancing ci processes.",2024-02-15
"37","assessing test artifact quality -- a tertiary study","huynh khanh vi tran, michael unterkalmsteiner, jürgen börstler, nauman bin ali","software engineering","context: modern software development increasingly relies on software testing for an ever more frequent delivery of high quality software. this puts high demands on the quality of the central artifacts in software testing, test suites and test cases. objective: we aim to develop a comprehensive model for capturing the dimensions of test case/suite quality, which are relevant for a variety of perspectives. method: we have carried out a systematic literature review to identify and analyze existing secondary studies on quality aspects of software testing artifacts. results: we identified 49 relevant secondary studies. of these 49 studies, less than half did some form of quality appraisal of the included primary studies and only 3 took into account the quality of the primary study when synthesizing the results. we present an aggregation of the context dimensions and factors that can be used to characterize the environment in which the test case/suite quality is investigated. we also provide a comprehensive model of test case/suite quality with definitions for the quality attributes and measurements based on findings in the literature and iso/iec 25010:2011. conclusion: the test artifact quality model presented in the paper can be used to support test artifact quality assessment and improvement initiatives in practice. furtherm information and software technology 139 (2021): 106620ore, the model can also be used as a framework for documenting context characteristics to make research results more accessible for research and practice.",2024-02-14
"38","trained without my consent: detecting code inclusion in language models trained on code","vahid majdinasab, amin nikanjam, foutse khomh","software engineering","code auditing ensures that the developed code adheres to standards, regulations, and copyright protection by verifying that it does not contain code from protected sources. the recent advent of large language models (llms) as coding assistants in the software development process poses new challenges for code auditing. the dataset for training these models is mainly collected from publicly available sources. this raises the issue of intellectual property infringement as developers' codes are already included in the dataset. therefore, auditing code developed using llms is challenging, as it is difficult to reliably assert if an llm used during development has been trained on specific copyrighted codes, given that we do not have access to the training datasets of these models. given the non-disclosure of the training datasets, traditional approaches such as code clone detection are insufficient for asserting copyright infringement. to address this challenge, we propose a new approach, trawic; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an llm's training dataset. we extract syntactic and semantic identifiers unique to each program to train a classifier for detecting code inclusion. in our experiments, we observe that trawic is capable of detecting 83.87% of codes that were used to train an llm. in comparison, the prevalent clone detection tool nicad is only capable of detecting 47.64%. in addition to its remarkable performance, trawic has low resource overhead in contrast to pair-wise clone detection that is conducted during the auditing process of tools like codewhisperer reference tracker, across thousands of code snippets.",2024-02-14
"39","context composing for full line code completion","anton semenkin, yaroslav sokolov, evgeniia vu","software engineering","code completion is one of the most used integrated development environment (ide) features, which affects the everyday life of a software developer. modern code completion approaches moved from the composition of several static analysis-based contributors to pipelines that involve neural networks. this change allows the proposal of longer code suggestions while maintaining the relatively short time spent on generation itself. at jetbrains, we put a lot of effort into perfecting the code completion workflow so it can be both helpful and non-distracting for a programmer. we managed to ship the full line code completion feature to pycharm pro ide and proved its usefulness in a/b testing on hundreds of real python users. the paper describes our approach to context composing for the transformer model that is a core of the feature's implementation. in addition to that, we share our next steps to improve the feature and emphasize the importance of several research aspects in the area.",2024-02-14
"40","generative ai for pull request descriptions: adoption, impact, and developer interventions","tao xiao, hideaki hata, christoph treude, kenichi matsumoto","software engineering","github's copilot for pull requests (prs) is a promising service aiming to automate various developer tasks related to prs, such as generating summaries of changes or providing complete walkthroughs with links to the relevant code. as this innovative technology gains traction in the open source software (oss) community, it is crucial to examine its early adoption and its impact on the development process. additionally, it offers a unique opportunity to observe how developers respond when they disagree with the generated content. in our study, we employ a mixed-methods approach, blending quantitative analysis with qualitative insights, to examine 18,256 prs in which parts of the descriptions were crafted by generative ai. our findings indicate that: (1) copilot for prs, though in its infancy, is seeing a marked uptick in adoption. (2) prs enhanced by copilot for prs require less review time and have a higher likelihood of being merged. (3) developers using copilot for prs often complement the automated descriptions with their manual input. these results offer valuable insights into the growing integration of generative ai in software development.",2024-02-14
"41","an evaluative comparison of performance portability across gpu programming models","joshua h. davis, pranav sivaraman, isaac minn, konstantinos parasyris, harshitha menon, giorgis georgakoudis, abhinav bhatele","distributed, parallel, and cluster computing","ensuring high productivity in scientific software development necessitates developing and maintaining a single codebase that can run efficiently on a range of accelerator-based supercomputing platforms. while prior work has investigated the performance portability of a few selected proxy applications or programming models, this paper provides a comprehensive study of a range of proxy applications implemented in the major programming models suitable for gpu-based platforms. we present and analyze performance results across nvidia and amd gpu hardware currently deployed in leadership-class computing facilities using a representative range of scientific codes and several programming models -- cuda, hip, kokkos, raja, openmp, openacc, and sycl. based on the specific characteristics of applications tested, we include recommendations to developers on how to choose the right programming model for their code. we find that kokkos and raja in particular offer the most promise empirically as performance portable programming models. these results provide a comprehensive evaluation of the extent to which each programming model for heterogeneous systems provides true performance portability in real-world usage.",2024-02-14
"42","chatgpt vs llama: impact, reliability, and challenges in stack overflow discussions","leuson da silva, jordan samhi, foutse khomh","software engineering","since its release in november 2022, chatgpt has shaken up stack overflow, the premier platform for developers' queries on programming and software development. demonstrating an ability to generate instant, human-like responses to technical questions, chatgpt has ignited debates within the developer community about the evolving role of human-driven platforms in the age of generative ai. two months after chatgpt's release, meta released its answer with its own large language model (llm) called llama: the race was on. we conducted an empirical study analyzing questions from stack overflow and using these llms to address them. this way, we aim to (ii) measure user engagement evolution with stack overflow over time; (ii) quantify the reliability of llms' answers and their potential to replace stack overflow in the long term; (iii) identify and understand why llms fails; and (iv) compare llms together. our empirical results are unequivocal: chatgpt and llama challenge human expertise, yet do not outperform it for some domains, while a significant decline in user posting activity has been observed. furthermore, we also discuss the impact of our findings regarding the usage and development of new llms.",2024-02-13
"43","on-the-fly syntax highlighting: generalisation and speed-ups","marco edoardo palma, alex wolf, pasquale salza, harald c. gall","software engineering","on-the-fly syntax highlighting is the task of rapidly associating visual secondary notation values with each character of a language derivation. research in this domain is driven by the prevalence of online software development tools, which frequently display source code on screen and heavily rely on syntax highlighting mechanisms. in this context, three contrasting demands confront resolvers in this space: speed, accuracy, and development costs. speed constraints are essential to ensure tool usability, manifesting as responsiveness for end users accessing online source code and minimising system overhead. simultaneously, achieving precise highlighting is critical for enhancing code comprehensibility. nevertheless, obtaining accurate results necessitates the capacity to perform grammatical analysis on the code under consideration, even in cases of varying grammatical correctness. furthermore, addressing the development costs of such resolvers is imperative, given the multitude of programming language versions. the current state-of-the-art approach in this field leverages the original lexer and parser of programming languages to create syntax highlighting oracles, subsequently used for training base recurrent neural network models. as the question of the generalisation of such a solution persists, this paper addresses this aspect by extending the original work to three additional mainstream programming languages and conducting a comprehensive review of the outcomes. moreover, the original limitations in evaluation performance and training costs are mitigated through the introduction of a novel convolutional based neural network model. this study examines the performance gains of running models on gpus, finding that the new cnn implementation is much faster than previous methods while maintaining high accuracy.",2024-02-13
"44","the current state of security -- insights from the german software industry","timo langstrof, alex r. sabau","cryptography and security","these days, software development and security go hand in hand. numerous techniques and strategies are discussed in the literature that can be applied to guarantee the incorporation of security into the software development process. in this paper the main ideas of secure software development that have been discussed in the literature are outlined. next, a dataset on implementation in practice is gathered through a qualitative interview research involving 20 companies. trends and correlations in this dataset are found and contrasted with theoretical ideas from the literature. the results show that the organizations that were polled are placing an increasing focus on security. although the techniques covered in the literature are being used in the real world, they are frequently not fully integrated into formal, standardized processes. the insights gained from our research lay the groundwork for future research, which can delve deeper into specific elements of these methods to enhance our understanding of their application in real-world scenarios.",2024-02-13
"45","what the fix? a study of asats rule documentation","corentin latappy, thomas degueule, jean-rémy falleri (labri), romain robbes (cnrs, labri, ub, bordeaux inp), xavier blanc, cédric teyton","software engineering","automatic static analysis tools (asats) are widely used by software developers to diffuse and enforce coding practices. yet, we know little about the documentation of asats, despite it being critical to learn about the coding practices in the first place. we shed light on this through several contributions. first, we analyze the documentation of more than 100 rules of 16 asats for multiple programming languages, and distill a taxonomy of the purposes of the documentation-what triggers a rule; why it is important; and how to fix an issue-and its types of contents. then, we conduct a survey to assess the effectiveness of the documentation in terms of its goals and types of content. we highlight opportunities for improvement in asat documentation. in particular, we find that the why purpose is missing in half of the rules we survey; moreover, when the why is present, it is more likely to have quality issues than the what and the fix.",2024-02-13
"46","turtlerabbit 2024 ssl team description paper","linh trinh, alif anzuman, eric batkhuu, dychen chan, lisa graf, darpan gurung, tharunimm jamal, jigme namgyal, jason ng, wing lam tsang, x. rosalind wang, eren yilmaz, oliver obst","robotics","turtlerabbit is a new robocup ssl team from western sydney university. this team description paper presents our approach in navigating some of the challenges in developing a new ssl team from scratch. ssl is dominated by teams with extensive experience and customised equipment that has been developed over many years. here, we outline our approach in overcoming some of the complexities associated with replicating advanced open-sourced designs and managing the high costs of custom components. opting for simplicity and cost-effectiveness, our strategy primarily employs off-the-shelf electronics components and ``hobby'' brushless direct current (bldc) motors, complemented by 3d printing and cnc milling. this approach helped us to streamline the development process and, with our open-sourced hardware design, hopefully will also lower the bar for other teams to enter robocup ssl in the future. the paper details the specific hardware choices, their approximate costs, the integration of electronics and mechanics, and the initial steps taken in software development, for our entry into ssl that aims to be simple yet competitive.",2024-02-13
"47","mercury: an efficiency benchmark for llm code synthesis","mingzhe du, anh tuan luu, bin ji, see-kiong ng","software engineering","despite advancements in evaluating large language models (llms) for code synthesis, benchmarks have predominantly focused on functional correctness, overlooking the importance of code efficiency. we present mercury, the first benchmark designated for assessing the code efficiency of llm code synthesis tasks. mercury consists of 1,889 programming tasks covering diverse difficulty levels alongside test case generators generating unlimited cases for comprehensive evaluation. unlike existing benchmarks, mercury integrates a novel metric beyond@k to measure normalized code efficiency based on historical submissions, leading to a new evaluation indicator for code synthesis, which encourages generating functionally correct and computationally efficient code, mirroring the real-world software development standard. our findings reveal that while llms demonstrate the remarkable capability to generate functionally correct code, there still exists a substantial gap in their efficiency output, underscoring a new frontier for llm research and development.",2024-02-12
"48","best practices for facing the security challenges of internet of things devices focusing on software development life cycle","md rafid islam, ratun rahman","software engineering","in the past few years, the number of iot devices has grown substantially, and this trend is likely to continue. an increasing amount of effort is being put into developing software for the ever-increasing iot devices. every iot system at its core has software that enables the devices to function efficiently. but security has always been a concern in this age of information and technology. security for iot devices is now a top priority due to the growing number of threats. this study introduces best practices for ensuring security in the iot, with an emphasis on guidelines to be utilized in software development for iot devices. the objective of the study is to raise awareness of potential threats, emphasizing the secure software development lifecycle. the study will also serve as a point of reference for future developments and provide a solid foundation for securing iot software and dealing with vulnerabilities.",2024-02-12
"49","asap-repair: api-specific automated program repair based on api usage graphs","sebastian nielebock, paul blockhaus, jacob krüger, frank ortmeier","software engineering","modern software development relies on the reuse of code via application programming interfaces (apis). such reuse relieves developers from learning and developing established algorithms and data structures anew, enabling them to focus on their problem at hand. however, there is also the risk of misusing an api due to a lack of understanding or proper documentation. while many techniques target api misuse detection, only limited efforts have been put into automatically repairing api misuses. in this paper, we present our advances on our technique api-specific automated program repair (asap-repair). asap-repair is intended to fix api misuses based on api usage graphs (augs) by leveraging api usage templates of state-of-the-art api misuse detectors. we demonstrate that asap-repair is in principle applicable on an established api misuse dataset. moreover, we discuss next steps and challenges to evolve asap-repair towards a full-fledged automatic program repair (apr) technique.",2024-02-12
"50","malicious package detection using metadata information","s. halder, m. bewong, a. mahboubi, y. jiang, r. islam, z. islam, r. ip, e. ahmed, g. ramachandran, a. babar","cryptography and security","protecting software supply chains from malicious packages is paramount in the evolving landscape of software development. attacks on the software supply chain involve attackers injecting harmful software into commonly used packages or libraries in a software repository. for instance, javascript uses node package manager (npm), and python uses python package index (pypi) as their respective package repositories. in the past, npm has had vulnerabilities such as the event-stream incident, where a malicious package was introduced into a popular npm package, potentially impacting a wide range of projects. as the integration of third-party packages becomes increasingly ubiquitous in modern software development, accelerating the creation and deployment of applications, the need for a robust detection mechanism has become critical. on the other hand, due to the sheer volume of new packages being released daily, the task of identifying malicious packages presents a significant challenge. to address this issue, in this paper, we introduce a metadata-based malicious package detection model, memptec. this model extracts a set of features from package metadata information. these extracted features are classified as either easy-to-manipulate (etm) or difficult-to-manipulate (dtm) features based on monotonicity and restricted control properties. by utilising these metadata features, not only do we improve the effectiveness of detecting malicious packages, but also we demonstrate its resistance to adversarial attacks in comparison with existing state-of-the-art. our experiments indicate a significant reduction in both false positives (up to 97.56%) and false negatives (up to 91.86%).",2024-02-12
"51","on (mis)perceptions of testing effectiveness: an empirical study","sira vegas, patricia riofrio, esperanza marcos, natalia juristo","software engineering","a recurring problem in software development is incorrect decision making on the techniques, methods and tools to be used. mostly, these decisions are based on developers' perceptions about them. a factor influencing people's perceptions is past experience, but it is not the only one. in this research, we aim to discover how well the perceptions of the defect detection effectiveness of different techniques match their real effectiveness in the absence of prior experience. to do this, we conduct an empirical study plus a replication. during the original study, we conduct a controlled experiment with students applying two testing techniques and a code review technique. at the end of the experiment, they take a survey to find out which technique they perceive to be most effective. the results show that participants' perceptions are wrong and that this mismatch is costly in terms of quality. in order to gain further insight into the results, we replicate the controlled experiment and extend the survey to include questions about participants' opinions on the techniques and programs. the results of the replicated study confirm the findings of the original study and suggest that participants' perceptions might be based not on their opinions about complexity or preferences for techniques but on how well they think that they have applied the techniques.",2024-02-11
"52","designing nlp-based solutions for requirements variability management: experiences from a design science study at visma","parisa elahidoost, michael unterkalmsteiner, davide fucci, peter liljenberg, jannik fischbach","software engineering","context and motivation: in this industry-academia collaborative project, a team of researchers, supported by a software architect, business analyst, and test engineer explored the challenges of requirement variability in a large business software development company. question/problem: following the design science paradigm, we studied the problem of requirements analysis and tracing in the context of contractual documents, with a specific focus on managing requirements variability. this paper reports on the lessons learned from that experience, highlighting the strategies and insights gained in the realm of requirements variability management. principal ideas/results: this experience report outlines the insights gained from applying design science in requirements engineering research in industry. we show and evaluate various strategies to tackle the issue of requirement variability. contribution: we report on the iterations and how the solution development evolved in parallel with problem understanding. from this process, we derive five key lessons learned to highlight the effectiveness of design science in exploring solutions for requirement variability in contract-based environments.",2024-02-11
"53","unprecedented code change automation: the fusion of llms and transformation by example","malinda dilhara, abhiram bellur, timofey bryksin, danny dig","software engineering","software developers often repeat code changes, known as ""code change patterns"" (cpats), within and across projects. automating these cpats accelerates development, but current transformation by example (tbe) techniques are limited by the input examples' quality and quantity, missing variations with different syntax or flow yet semantically similar. large language models (llms), trained on vast code datasets, can overcome these limitations by generating semantically equivalent, unseen cpat variants, enhancing tbe effectiveness.
we identified best practices for using llms to generate code variants meeting criteria of correctness, usefulness, and applicability. implementing these in pycraft, combining static and dynamic analysis with llms, we achieved an f-measure of 96.6% in identifying correct variants, expanding inputs by 58x on average, and automating changes to increase target codes by up to 39x. patches from pycraft were submitted to projects like microsoft/deepspeed and ibm/infairness, with an 83% acceptance rate, validating our approach's usefulness.",2024-02-11
"54","using large language models for student-code guided test case generation in computer science education","nischal ashok kumar, andrew lan","computation and language","in computer science education, test cases are an integral part of programming assignments since they can be used as assessment items to test students' programming knowledge and provide personalized feedback on student-written code. the goal of our work is to propose a fully automated approach for test case generation that can accurately measure student knowledge, which is important for two reasons. first, manually constructing test cases requires expert knowledge and is a labor-intensive process. second, developing test cases for students, especially those who are novice programmers, is significantly different from those oriented toward professional-level software developers. therefore, we need an automated process for test case generation to assess student knowledge and provide feedback. in this work, we propose a large language model-based approach to automatically generate test cases and show that they are good measures of student knowledge, using a publicly available dataset that contains student-written java code. we also discuss future research directions centered on using test cases to help students.",2024-02-11
"55","exploring interaction patterns for debugging: enhancing conversational capabilities of ai-assistants","bhavya chopra, yasharth bajpai, param biyani, gustavo soares, arjun radhakrishna, chris parnin, sumit gulwani","human-computer interaction","the widespread availability of large language models (llms) within integrated development environments (ides) has led to their speedy adoption. conversational interactions with llms enable programmers to obtain natural language explanations for various software development tasks. however, llms often leap to action without sufficient context, giving rise to implicit assumptions and inaccurate responses. conversations between developers and llms are primarily structured as question-answer pairs, where the developer is responsible for asking the the right questions and sustaining conversations across multiple turns. in this paper, we draw inspiration from interaction patterns and conversation analysis -- to design robin, an enhanced conversational ai-assistant for debugging. through a within-subjects user study with 12 industry professionals, we find that equipping the llm to -- (1) leverage the insert expansion interaction pattern, (2) facilitate turn-taking, and (3) utilize debugging workflows -- leads to lowered conversation barriers, effective fault localization, and 5x improvement in bug resolution rates.",2024-02-09
"56","rocks coding, not development--a human-centric, experimental evaluation of llm-supported se tasks","wei wang, huilong ning, gaowei zhang, libo liu, yi wang","software engineering","recently, large language models (llm) based generative ai has been gaining momentum for their impressive high-quality performances in multiple domains, particularly after the release of the chatgpt. many believe that they have the potential to perform general-purpose problem-solving in software development and replace human software developers. nevertheless, there are in a lack of serious investigation into the capability of these llm techniques in fulfilling software development tasks. in a controlled 2 x 2 between-subject experiment with 109 participants, we examined whether and to what degree working with chatgpt was helpful in the coding task and typical software development task and how people work with chatgpt. we found that while chatgpt performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good. we also observed the interactions between participants and chatgpt and found the relations between the interactions and the outcomes. our study thus provides first-hand insights into using chatgpt to fulfill software engineering tasks with real-world developers and motivates the need for novel interaction mechanisms that help developers effectively work with large language models to achieve desired outcomes.",2024-02-08
"57","the impact of ai tool on engineering at anz bank an emperical study on github copilot within coporate environment","sayan chatterjee, ching louis liu, gareth rowland, tim hogarth","software engineering","the increasing popularity of ai, particularly large language models (llms), has significantly impacted various domains, including software engineering. this study explores the integration of ai tools in software engineering practices within a large organization. we focus on anz bank, which employs over 5000 engineers covering all aspects of the software development life cycle. this paper details an experiment conducted using github copilot, a notable ai tool, within a controlled environment to evaluate its effectiveness in real-world engineering tasks. additionally, this paper shares initial findings on the productivity improvements observed after github copilot was adopted on a large scale, with about 1000 engineers using it. anz bank's six-week experiment with github copilot included two weeks of preparation and four weeks of active testing. the study evaluated participant sentiment and the tool's impact on productivity, code quality, and security. initially, participants used github copilot for proposed use-cases, with their feedback gathered through regular surveys. in the second phase, they were divided into control and copilot groups, each tackling the same python challenges, and their experiences were again surveyed. results showed a notable boost in productivity and code quality with github copilot, though its impact on code security remained inconclusive. participant responses were overall positive, confirming github copilot's effectiveness in large-scale software engineering environments. early data from 1000 engineers also indicated a significant increase in productivity and job satisfaction.",2024-02-08
"58","polaris: a framework to guide the development of trustworthy ai systems","maria teresa baldassarre, domenico gigante, marcos kalinowski, azzurra ragone","software engineering","in the ever-expanding landscape of artificial intelligence (ai), where innovation thrives and new products and services are continuously being delivered, ensuring that ai systems are designed and developed responsibly throughout their entire lifecycle is crucial. to this end, several ai ethics principles and guidelines have been issued to which ai systems should conform. nevertheless, relying solely on high-level ai ethics principles is far from sufficient to ensure the responsible engineering of ai systems. in this field, ai professionals often navigate by sight. indeed, while recommendations promoting trustworthy ai (tai) exist, these are often high-level statements that are difficult to translate into concrete implementation strategies. there is a significant gap between high-level ai ethics principles and low-level concrete practices for ai professionals. to address this challenge, our work presents an experience report where we develop a novel holistic framework for trustworthy ai - designed to bridge the gap between theory and practice - and report insights from its application in an industrial case study. the framework is built on the result of a systematic review of the state of the practice, a survey, and think-aloud interviews with 34 ai practitioners. the framework, unlike most of those already in the literature, is designed to provide actionable guidelines and tools to support different types of stakeholders throughout the entire software development life cycle (sdlc). our goal is to empower ai professionals to confidently navigate the ethical dimensions of tai through practical insights, ensuring that the vast potential of ai is exploited responsibly for the benefit of society as a whole.",2024-02-08
"59","ten simple rules for teaching sustainable software engineering","kit gallagher, richard creswell, ben lambert, martin robinson, chon lok lei, gary r. mirams, david j. gavaghan","computers and society","computational methods and associated software implementations are central to every field of scientific investigation. modern biological research, particularly within systems biology, has relied heavily on the development of software tools to process and organize increasingly large datasets, simulate complex mechanistic models, provide tools for the analysis and management of data, and visualize and organize outputs. however, developing high-quality research software requires scientists to develop a host of software development skills, and teaching these skills to students is challenging. there has been a growing importance placed on ensuring reproducibility and good development practices in computational research. however, less attention has been devoted to informing the specific teaching strategies which are effective at nurturing in researchers the complex skillset required to produce high-quality software that, increasingly, is required to underpin both academic and industrial biomedical research. recent articles in the ten simple rules collection have discussed the teaching of foundational computer science and coding techniques to biology students. we advance this discussion by describing the specific steps for effectively teaching the necessary skills scientists need to develop sustainable software packages which are fit for (re-)use in academic research or more widely. although our advice is likely to be applicable to all students and researchers hoping to improve their software development skills, our guidelines are directed towards an audience of students that have some programming literacy but little formal training in software development or engineering, typical of early doctoral students. these practices are also applicable outside of doctoral training environments, and we believe they should form a key part of postgraduate training schemes more generally in the life sciences.",2024-02-07
"60","the invisible game on the internet: a case study of decoding deceptive patterns","zewei shi, ruoxi sun, jieshan chen, jiamou sun, minhui xue","cryptography and security","deceptive patterns are design practices embedded in digital platforms to manipulate users, representing a widespread and long-standing issue in the web and mobile software development industry. legislative actions highlight the urgency of globally regulating deceptive patterns. however, despite advancements in detection tools, a significant gap exists in assessing deceptive pattern risks. in this study, we introduce a comprehensive approach involving the interactions between the adversary, watchdog (e.g., detection tools), and challengers (e.g., users) to formalize and decode deceptive pattern threats. based on this, we propose a quantitative risk assessment system. representative cases are analyzed to showcase the practicability of the proposed risk scoring system, emphasizing the importance of involving human factors in deceptive pattern risk assessment.",2024-02-05
"61","dynamic test case prioritization in industrial test result datasets","alina torbunova, per erik strandberg, ivan porres","software engineering","regression testing in software development checks if new software features affect existing ones. regression testing is a key task in continuous development and integration, where software is built in small increments and new features are integrated as soon as possible. it is therefore important that developers are notified about possible faults quickly. in this article, we propose a test case prioritization schema that combines the use of a static and a dynamic prioritization algorithm. the dynamic prioritization algorithm rearranges the order of execution of tests on the fly, while the tests are being executed. we propose to use a conditional probability dynamic algorithm for this. we evaluate our solution on three industrial datasets and utilize average percentage of fault detection for that. the main findings are that our dynamic prioritization algorithm can: a) be applied with any static algorithm that assigns a priority score to each test case b) can improve the performance of the static algorithm if there are failure correlations between test cases c) can also reduce the performance of the static algorithm, but only when the static scheduling is performed at a near optimal level.",2024-02-05
"62","fair-use4os: from open source to open source","raphael sonabend, hugo gruson, leo wolansky, agnes kiragga, daniel s. katz","software engineering","this paper extends the fair (findable, accessible, interoperable, reusable) guidelines to provide criteria for assessing if software is open source. by adding 'use' (user-centered, sustainable, equitable), software development can adhere to open source best practice by incorporating user-input early on, ensuring front-end designs are accessible to all possible stakeholders, and planning long-term sustainability alongside software design. the fair-use4os guidelines will allow funders and researchers to more effectively evaluate and plan open source software projects. there is good evidence of funders increasingly mandating that all funded research software is open-source; however, even under the fair guidelines, this could simply mean software released on github with a zenodo doi. by employing the fair-use4os guidelines, best practice can be demonstrated from the very beginning of the design process and the software has the greatest chance of success by being truly 'open source'.",2024-02-05
"63","how do software practitioners perceive human-centric defects?","vedant chauhan, chetan arora, hourieh khalajzadeh, john grundy","software engineering","context: human-centric software design and development focuses on how users want to carry out their tasks rather than making users accommodate their software. software users can have different genders, ages, cultures, languages, disabilities, socioeconomic statuses, and educational backgrounds, among many other differences. due to the inherently varied nature of these differences and their impact on software usage, preferences and issues of users can vary, resulting in user-specific defects that we term as `human-centric defects' (hcds).
objective: this research aims to understand the perception and current management practices of such human-centric defects by software practitioners, identify key challenges in reporting, understanding and fixing them, and provide recommendations to improve hcds management in software engineering.
method: we conducted a survey and interviews with software engineering practitioners to gauge their knowledge and experience on hcds and the defect tracking process.
results: we analysed fifty (50) survey- and ten (10) interview- responses from se practitioners and identified that there are multiple gaps in the current management of hcds in software engineering practice. there is a lack of awareness regarding human-centric aspects, causing them to be lost or under-appreciated during software development. our results revealed that handling hcds could be improved by following a better feedback process with end-users, a more descriptive taxonomy, and suitable automation.
conclusion: hcds present a major challenge to software practitioners, given their diverse end-user base. in the software engineering domain, research on hcds has been limited and requires effort from the research and practice communities to create better awareness and support regarding human-centric aspects.",2024-02-05
"64","improving the learning of code review successive tasks with cross-task knowledge distillation","oussama ben sghaier, houari sahraoui","software engineering","code review is a fundamental process in software development that plays a pivotal role in ensuring code quality and reducing the likelihood of errors and bugs. however, code review can be complex, subjective, and time-consuming. quality estimation, comment generation, and code refinement constitute the three key tasks of this process, and their automation has traditionally been addressed separately in the literature using different approaches. in particular, recent efforts have focused on fine-tuning pre-trained language models to aid in code review tasks, with each task being considered in isolation. we believe that these tasks are interconnected, and their fine-tuning should consider this interconnection. in this paper, we introduce a novel deep-learning architecture, named discorev, which employs cross-task knowledge distillation to address these tasks simultaneously. in our approach, we utilize a cascade of models to enhance both comment generation and code refinement models. the fine-tuning of the comment generation model is guided by the code refinement model, while the fine-tuning of the code refinement model is guided by the quality estimation model. we implement this guidance using two strategies: a feedback-based learning objective and an embedding alignment objective. we evaluate discorev by comparing it to state-of-the-art methods based on independent training and fine-tuning. our results show that our approach generates better review comments, as measured by the bleu score, as well as more accurate code refinement according to the codebleu score",2024-02-03
"65","effibench: benchmarking the efficiency of automatically generated code","dong huang, jie m.zhang, yuhao qing, heming cui","software engineering","code generation models have increasingly become integral to aiding software development, offering assistance in tasks such as code completion, debugging, and code translation. although current research has thoroughly examined the correctness of code produced by code generation models, a vital aspect, i.e., the efficiency of the generated code, has often been neglected. this paper presents effibench, a benchmark with 1,000 efficiency-critical coding problems for assessing the efficiency of code generated by code generation models. effibench contains a diverse set of leetcode coding problems. each problem is paired with an executable human-written canonical solution. with effibench, we empirically examine the capability of 21 large language models (13 open-sourced and 8 closed-sourced) in generating efficient code. the results demonstrate that gpt-4-turbo generates the most efficient code, significantly outperforming palm-2-chat-bison, claude-instant-1, gemini-pro, gpt-4, and gpt-3.5. nevertheless, its code efficiency is still worse than the efficiency of human-written canonical solutions. in particular, the average and worst execution time of gpt-4-turbo generated code is 1.69 and 45.49 times that of the canonical solutions.",2024-02-03
"66","understanding the building blocks of accountability in software engineering","adam alami, neil ernst","software engineering","in the social and organizational sciences, accountability has been linked to the efficient operation of organizations. however, it has received limited attention in software engineering (se) research, in spite of its central role in the most popular software development methods (e.g., scrum). in this article, we explore the mechanisms of accountability in se environments. we investigate the factors that foster software engineers' individual accountability within their teams through an interview study with 12 people. our findings recognize two primary forms of accountability shaping software engineers individual senses of accountability: institutionalized and grassroots. while the former is directed by formal processes and mechanisms, like performance reviews, grassroots accountability arises organically within teams, driven by factors such as peers' expectations and intrinsic motivation. this organic form cultivates a shared sense of collective responsibility, emanating from shared team standards and individual engineers' inner commitment to their personal, professional values, and self-set standards. while institutionalized accountability relies on traditional ""carrot and stick"" approaches, such as financial incentives or denial of promotions, grassroots accountability operates on reciprocity with peers and intrinsic motivations, like maintaining one's reputation in the team.",2024-02-02
"67","comet: generating commit messages using delta graph context representation","abhinav reddy mandli, saurabhsingh rajput, tushar sharma","software engineering","commit messages explain code changes in a commit and facilitate collaboration among developers. several commit message generation approaches have been proposed; however, they exhibit limited success in capturing the context of code changes. we propose comet (context-aware commit message generation), a novel approach that captures context of code changes using a graph-based representation and leverages a transformer-based model to generate high-quality commit messages. our proposed method utilizes delta graph that we developed to effectively represent code differences. we also introduce a customizable quality assurance module to identify optimal messages, mitigating subjectivity in commit messages. experiments show that comet outperforms state-of-the-art techniques in terms of bleu-norm and meteor metrics while being comparable in terms of rogue-l. additionally, we compare the proposed approach with the popular gpt-3.5-turbo model, along with gpt-4-turbo; the most capable gpt model, over zero-shot, one-shot, and multi-shot settings. we found comet outperforming the gpt models, on five and four metrics respectively and provide competitive results with the two other metrics. the study has implications for researchers, tool developers, and software developers. software developers may utilize comet to generate context-aware commit messages. researchers and tool developers can apply the proposed delta graph technique in similar contexts, like code review summarization.",2024-02-02
"68","codepori: large scale model for autonomous software development by using multi-agents","zeeshan rasheed, muhammad waseem, mika saari, kari systä, pekka abrahamsson","software engineering","large language models (llms) and generative pre-trained transformers (gpts) are reshaping the field of software engineering (se). existing llm-based multi-agent systems have successfully resolved simple dialogue tasks. however, the potential of llms for more complex tasks, such as automated code generation for large and complex projects, have been explored in only a few existing works. this paper introduces codepori, a novel model designed to automate code generation for extensive and complex software projects based on natural language prompts. we employ llm-based multi-ai agents to handle creative and challenging tasks in autonomous software development. each agent engages with a specific task, including system design, code development, code review, code verification, and test engineering. we show in the paper that codepori is able to generate running code for large-scale projects, completing the entire software development process in minutes rather than hours, and at a cost of a few dollars. it identifies and mitigates potential security vulnerabilities and corrects errors while maintaining a solid code performance level. we also conducted an evaluation of codepori against existing solutions using humaneval and the massively multitask benchmark for python (mbpp) benchmark. the results indicate that codepori improves upon the benchmarks in terms of code accuracy, efficiency, and overall performance. for example, codepori improves the pass@1 metric on humaneval to 87.5% and on mbpp to 86.5%, representing a clear improvement over the existing models. we also assessed codepori's performance through practitioner evaluations, with 91% expressing satisfaction with the model's performance.",2024-02-02
"69","can large language models serve as data analysts? a multi-agent assisted approach for qualitative data analysis","zeeshan rasheed, muhammad waseem, aakash ahmad, kai-kristian kemell, wang xiaofeng, anh nguyen duc, pekka abrahamsson","software engineering","recent advancements in large language models (llms) have enabled collaborative human-bot interactions in software engineering (se), similar to many other professions. however, the potential benefits and implications of incorporating llms into qualitative data analysis in se have not been completely explored. for instance, conducting qualitative data analysis manually can be a time-consuming, effort-intensive, and error-prone task for researchers. llm-based solutions, such as generative ai models trained on massive datasets, can be utilized to automate tasks in software development as well as in qualitative data analysis. to this end, we utilized llms to automate and expedite the qualitative data analysis processes. we employed a multi-agent model, where each agent was tasked with executing distinct, individual research related activities. our proposed model interpreted large quantities of textual documents and interview transcripts to perform several common tasks used in qualitative analysis. the results show that this technical assistant speeds up significantly the data analysis process, enabling researchers to manage larger datasets much more effectively. furthermore, this approach introduces a new dimension of scalability and accuracy in qualitative research, potentially transforming data interpretation methodologies in se.",2024-02-02
"70","an empirical study on low code programming using traditional vs large language model support","yongkun liu, jiachi chen, tingting bi, john grundy, yanlin wang, ting chen, yutian tang, zibin zheng","software engineering","low-code programming (lcp) refers to programming using models at higher levels of abstraction, resulting in less manual and more efficient programming, and reduced learning effort for amateur developers. many lcp tools have rapidly evolved and have benefited from the concepts of visual programming languages (vpls) and programming by demonstration (pbd). with huge increase in interest in using large language models (llms) in software engineering, llm-based lcp has began to become increasingly important. however, the technical principles and application scenarios of traditional approaches to lcp and llm-based lcp are significantly different. understanding these key differences and characteristics in the application of the two approaches to lcp by users is crucial for lcp providers in improving existing and developing new lcp tools, and in better assisting users in choosing the appropriate lcp technology. we conducted an empirical study of both traditional lcp and llm-based lcp. we analyzed developers' discussions on stack overflow (so) over the past three years and then explored the similarities and differences between traditional lcp and llm-based lcp features and developer feedback. our findings reveal that while traditional lcp and llm-based lcp share common primary usage scenarios, they significantly differ in scope, limitations and usage throughout the software development lifecycle, particularly during the implementation phase. we also examine how llms impact and integrate with lcp, discussing the latest technological developments in llm-based lcp, such as its integration with vpls and the application of llm agents in software engineering.",2024-02-02
"71","towards understanding the challenges of bug localization in deep learning systems","sigma jahan, mehil b. shah, mohammad masudur rahman","software engineering","software bugs cost the global economy billions of dollars annually and claim ~50\% of the programming time from software developers. locating these bugs is crucial for their resolution but challenging. it is even more challenging in deep-learning systems due to their black-box nature. bugs in these systems are also hidden not only in the code but also in the models and training data, which might make traditional debugging methods less effective. in this article, we conduct a large-scale empirical study to better understand the challenges of localizing bugs in deep-learning systems. first, we determine the bug localization performance of four existing techniques using 2,365 bugs from deep-learning systems and 2,913 from traditional software. we found these techniques significantly underperform in localizing deep-learning system bugs. second, we evaluate how different bug types in deep learning systems impact bug localization. we found that the effectiveness of localization techniques varies with bug type due to their unique challenges. for example, tensor bugs were more accessible to locate due to their structural nature, while all techniques struggled with gpu bugs due to their external dependencies. third, we investigate the impact of bugs' extrinsic nature on localization in deep-learning systems. we found that deep learning bugs are often extrinsic and thus connected to artifacts other than source code (e.g., gpu, training data), contributing to the poor performance of existing localization methods.",2024-02-01
"72","data management challenges in agile software projects: a systematic literature review","ahmed fawzy, amjed tahir, matthias galster, peng liang","software engineering","agile software development follows an adaptive and iterative approach. however, the management of data (e.g., development data or product data) can pose significant challenges for projects and agile teams. we aim to identify and characterize key challenges faced in data management within agile projects and to examine potential solutions proposed in the literature. we used a systematic literature review (slr) to collect and analyse relevant studies. we identified 45 studies related to data management in agile software development. we then manually analysed and mapped data from these studies to categorise different data management aspects and identify challenges and solutions as identified in those studies. our findings reveal major challenges such as data integration and quality assurance. we found implications of challenges on team members and the product delivery process. we found that teams frequently struggle to integrate heterogeneous data sources, ensuring data reliability and real-time analytics. additionally, fragmented data collection and a lack of standardized practices can impede team collaboration and project transparency. the studies have also proposed various solutions to address those challenges, including the use of ontologies, diverse data management strategies, automated tools, and the adoption of quality-focused development methods. solutions also include training to enhance data quality and analysis. this slr provides in-depth insights and recommendations for practitioners, emphasizing the importance of robust data management strategies. it suggests integrating advanced data management techniques into agile frameworks to enhance decision-making and improve software project outcomes. the study highlights the need for a more focused approach to data management in agile environments, advocating tailored solutions to meet the unique demands of agile software development.",2024-02-01
"73","large language models based fuzzing techniques: a survey","linghan huang, peizhou zhao, huaming chen, lei ma","software engineering","in the modern era where software plays a pivotal role, software security and vulnerability analysis have become essential for software development. fuzzing test, as an efficient software testing method, are widely used in various domains. moreover, the rapid development of large language models (llms) has facilitated their application in the field of software testing, demonstrating remarkable performance. considering that existing fuzzing test techniques are not entirely automated and software vulnerabilities continue to evolve, there is a growing trend towards employing fuzzing test generated based on large language models. this survey provides a systematic overview of the approaches that fuse llms and fuzzing tests for software testing. in this paper, a statistical analysis and discussion of the literature in three areas, namely llms, fuzzing test, and fuzzing test generated based on llms, are conducted by summarising the state-of-the-art methods up until 2024. our survey also investigates the potential for widespread deployment and application of fuzzing test techniques generated by llms in the future.",2024-02-01
"74","an architecture for software engineering gamification","óscar pedreira, félix garcía, mario piattini, alejandro cortiñas, ana cerdeira-pena","software engineering","gamification has been applied in software engineering to improve quality and results by increasing people's motivation and engagement. a systematic mapping has identified research gaps in the field, one of them being the difficulty of creating an integrated gamified environment comprising all the tools of an organization, since most existing gamified tools are custom developments or prototypes. in this paper, we propose a gamification software architecture that allows us to transform the work environment of a software organization into an integrated gamified environment, i.e., the organization can maintain its tools, and the rewards obtained by the users for their actions in different tools will mount up. we developed a gamification engine based on our proposal, and we carried out a case study in which we applied it in a real software development company. the case study shows that the gamification engine has allowed the company to create a gamified workplace by integrating custom developed tools and off-the-shelf tools such as redmine, testlink, or junit, with the gamification engine. two main advantages can be highlighted: (i) our solution allows the organization to maintain its current tools, and (ii) the rewards for actions in any tool accumulate in a centralized gamified environment.",2024-01-31
"75","interoperable workflows by exchanging grid-based data between quantum-chemical program packages","kevin focke, matteo de santis, mario wolter, jessica a. martinez b, valérie vallet, andré severo pereira gomes, małgorzata olejniczak, christoph r. jacob","chemical physics","quantum-chemical subsystem and embedding methods require complex workflows that may involve multiple quantum-chemical program packages. moreover, such workflows require the exchange of voluminous data that goes beyond simple quantities such as molecular structures and energies. here, we describe our approach for addressing this interoperability challenge by exchanging electron densities and embedding potentials as grid-based data. we describe the approach that we have implemented to this end in a dedicated code, pyembed, currently part of a python scripting framework. we discuss how it has facilitated the development of quantum-chemical subsystem and embedding methods, and highlight several applications that have been enabled by pyembed, including wft-in-dft embedding schemes mixing non-relativistic and relativistic electronic structure methods, real-time time-dependent dft-in-dft approaches, the density-based many-body expansion, and workflows including real-space data analysis and visualization. our approach demonstrates in particular the merits of exchanging (complex) grid-based data, and in general the potential of modular software development in quantum chemistry, which hinges upon libraries that facilitate interoperability.",2024-01-31
"76","gamifying a software testing course with continuous integration","philipp straubinger, gordon fraser","software engineering","testing plays a crucial role in software development, and it is essential for software engineering students to receive proper testing education. however, motivating students to write tests and use automated testing during software development can be challenging. to address this issue and enhance student engagement in testing when they write code, we propose to incentivize students to test more by gamifying continuous integration. for this we use gamekins, a tool that is seamlessly integrated into the jenkins continuous integration platform and uses game elements based on commits to the source code repository: developers can earn points by completing test challenges and quests generated by gamekins, compete with other developers or teams on a leaderboard, and receive achievements for their test-related accomplishments. in this paper, we present our integration of gamekins into an undergraduate-level course on software testing. we observe a correlation between how students test their code and their use of gamekins, as well as a significant improvement in the accuracy of their results compared to a previous iteration of the course without gamification. as a further indicator of how this approach improves testing behavior, the students reported enjoyment in writing tests with gamekins.",2024-01-31
"77","challenges in understanding the relationship between teamwork quality and project success in large-scale agile projects","torgeir dingsøyr, phillip schneider, gunnar rye bergersen, yngve lindsjørn","software engineering","a number of methods for large-scale agile development have recently been suggested. much of the advice in agile methods focuses on teamwork. prior research has established that teamwork quality influences project success both for traditional software development teams and agile teams. further, prior studies have also suggested that teamwork quality may play out differently in large projects compared to small. we investigated the relationship between teamwork quality and project success with a survey of 196 project participants across 34 teams in four projects, replicating a previous study on single teams. the new data do not fit the previously established theoretical model, which raises several concerns. the observed effect of teamwork quality on project success operates differently across projects. we discuss possible reasons, which include disagreements on what characterises success in large-scale agile development, ""concept drift"" of teamwork quality factors, the possibility that interteam factors might have more influence on project success than intrateam factors, and finally, that our study design does not capture all relevant levels and functions. we conclude with a call for more studies on the quality and frequency of interaction between teams in addition to internal team factors to further advance theory and practice within large-scale agile software development.",2024-01-31
"78","spviz: a dsl-driven approach for software project visualization tooling","niklas rentz, reinhard von hanxleden","software engineering","for most service architectures, such as osgi and spring, architecture-specific tools allow software developers and architects to visualize otherwise obscure configurations hidden in the project files. such visualization tools are often used for documentation purposes and help to better understand programs than with source code alone. however, such tools often do not address project-specific peculiarities or do not exist at all for less common architectures, requiring developers to use different visualization and analysis tools within the same architecture. furthermore, many generic modeling tools and architecture visualization tools require their users to create and maintain models manually.
we here propose a dsl-driven approach that allows software architects to define and adapt their own project visualization tool. the approach, which we refer to as software project visualization (spviz), uses two dsls, one to describe architectural elements and their relationships, and one to describe how these should be visualized. we demonstrate how spviz can then automatically synthesize a customized, project-specific visualization tool that can adapt to changes in the underlying project automatically.
we implemented our approach in an open-source library, also termed spviz and discuss and analyze four different tools that follow this concept, including open-source projects and projects from an industrial partner in the railway domain.",2024-01-30
"79","llm4sechw: leveraging domain specific large language model for hardware debugging","weimin fu, kaichen yang, raj gautam dutta, xiaolong guo, gang qu","hardware architecture","this paper presents llm4sechw, a novel framework for hardware debugging that leverages domain specific large language model (llm). despite the success of llms in automating various software development tasks, their application in the hardware security domain has been limited due to the constraints of commercial llms and the scarcity of domain specific data. to address these challenges, we propose a unique approach to compile a dataset of open source hardware design defects and their remediation steps, utilizing version control data. this dataset provides a substantial foundation for training machine learning models for hardware. llm4sechw employs fine tuning of medium sized llms based on this dataset, enabling the identification and rectification of bugs in hardware designs. this pioneering approach offers a reference workflow for the application of fine tuning domain specific llms in other research areas. we evaluate the performance of our proposed system on various open source hardware designs, demonstrating its efficacy in accurately identifying and correcting defects. our work brings a new perspective on automating the quality control process in hardware design.",2024-01-28
"80","the role of library versions in developer-chatgpt conversations","rachna raj, diego elias costa","software engineering","the latest breakthroughs in large language models (llm) have empowered software development tools, such as chatgpt, to aid developers in complex tasks. developers use chatgpt to write code, review code changes, and even debug their programs. in these interactions, chatgpt often recommends code snippets that depend on external libraries. however, code from libraries changes over time, invalidating a once-correct code snippet and making it difficult to reuse recommended code.
in this study, we analyze devgpt, a dataset of more than 4,000 developer-chatgpt interactions, to understand the role of library versions in code-related conversations. we quantify how often library version constraints are mentioned in code-related conversations and when chatgpt recommends the installation of specific libraries. our findings show that, albeit to constantly recommend and analyze code with external dependencies, library version constraints only appear in 9% of the conversations. in the majority of conversations, the version constraints are prompted by users (as opposed to being specified by chatgpt) as a method for receiving better quality responses. moreover, we study how library version constraints are used in the conversation through qualitative methods, identifying several potential problems that warrant further research.",2024-01-29
"81","security code review by llms: a deep dive into responses","jiaxin yu, peng liang, yujia fu, amjed tahir, mojtaba shahin, chong wang, yangxiao cai","software engineering","security code review aims to combine automated tools and manual efforts to detect security defects during development. the rapid development of large language models (llms) has shown promising potential in software development, as well as opening up new possibilities in automated security code review. to explore the challenges of applying llms in practical code review for security defect detection, this study compared the detection performance of three state-of-the-art llms (gemini pro, gpt-4, and gpt-3.5) under five prompts on 549 code files that contain security defects from real-world code reviews. through analyzing 82 responses generated by the best-performing llm-prompt combination based on 100 randomly selected code files, we extracted and categorized quality problems present in these responses into 5 themes and 16 categories. our results indicate that the responses produced by llms often suffer from verbosity, vagueness, and incompleteness, highlighting the necessity to enhance their conciseness, understandability, and compliance to security defect detection. this work reveals the deficiencies of llm-generated responses in security code review and paves the way for future optimization of llms towards this task.",2024-01-29
"82","an empirical study on usage and perceptions of llms in a software engineering project","sanka rasnayaka, guanlin wang, ridwan shariffdeen, ganesh neelakanta iyer","software engineering","large language models (llms) represent a leap in artificial intelligence, excelling in tasks using human language(s). although the main focus of general-purpose llms is not code generation, they have shown promising results in the domain. however, the usefulness of llms in an academic software engineering project has not been fully explored yet. in this study, we explore the usefulness of llms for 214 students working in teams consisting of up to six members. notably, in the academic course through which this study is conducted, students were encouraged to integrate llms into their development tool-chain, in contrast to most other academic courses that explicitly prohibit the use of llms.
in this paper, we analyze the ai-generated code, prompts used for code generation, and the human intervention levels to integrate the code into the code base. we also conduct a perception study to gain insights into the perceived usefulness, influencing factors, and future outlook of llm from a computer science student's perspective. our findings suggest that llms can play a crucial role in the early stages of software development, especially in generating foundational code structures, and helping with syntax and error debugging. these insights provide us with a framework on how to effectively utilize llms as a tool to enhance the productivity of software engineering students, and highlight the necessity of shifting the educational focus toward preparing students for successful human-ai collaboration.",2024-01-29
"83","agile effort estimation: comparing the accuracy and efficiency of planning poker, bucket system, and affinity estimation methods","marko poženel, luka fürst, damjan vavpotič, tomaž hovelja","software engineering","published studies on agile effort estimation predominantly focus on comparisons of the accuracy of different estimation methods, while efficiency comparisons, i.e. how much time the estimation methods consume was not in the forefront. however, for practical use in software development, the time required can be a very important cost factor for enterprises, especially when the accuracy of different agile effort estimations is similar. in this study, we thus try to advance the current standard accuracy comparison between methods by introducing efficiency i.e. time it takes to use a method as an additional dimension of comparison. we conduct this comparison between three agile effort estimation methods that were not yet compared in the literature, namely planning poker, bucket system and affinity estimation. for the comparison, we used eight student teams with 29 students that had to use all the effort estimation methods during the course where they had to finish a programming project in 3 weeks. the results indicate that after the students get used to using the different methods the accuracy between them is not statistically significantly different, however, the efficiency is. on average bucket system and affinity estimation methods take half as much time as planning poker.",2024-01-29
"84","jtag-based remote configuration of fpgas over optical fibers","binwei deng, chonghan liu, jinghong chen, kai chen, datao gong, di guo, suen hou, deping huang, xiaoting li, tiankuan liu, ping-kun teng, annie c. xiang, hao xu, yang you, jingbo ye","instrumentation and detectors","in this paper, a remote fpga-configuration method based on jtag extension over optical fibers is presented. the method takes advantage of commercial components and ready-to-use software such as impact and does not require any hardware or software development. the method combines the advantages of the slow remote jtag configuration and the fast local flash memory configuration. the method has been verified successfully and used in the demonstrator of liquid-argon trigger digitization board (ltdb) for the atlas liquid argon calorimeter phase-i trigger upgrade. all components on the fpga side are verified to meet the radiation tolerance requirements.",2024-01-28
"85","where's the ""up""?! a comprehensive (bottom-up) study on the security of arm cortex-m systems","xi tan, zheyuan ma, sandro pinto, le guan, ning zhang, jun xu, zhiqiang lin, hongxin hu, ziming zhao","cryptography and security","arm cortex-m processors are the most widely used 32-bit microcontrollers among embedded and internetof-things devices. despite the widespread usage, there has been little effort in summarizing their hardware security features, characterizing the limitations and vulnerabilities of their hardware and software stack, and systematizing the research on securing these systems. the goals and contributions of this paper are multi-fold. first, we analyze the hardware security limitations and issues of cortex-m systems. second, we conducted a deep study of the software stack designed for cortex-m and revealed its limitations, which is accompanied by an empirical analysis of 1,797 real-world firmware from seven hardware vendors. third, we categorize the reported bugs in cortex-m software systems. finally, we systematize the efforts that aim at securing cortex-m systems and evaluate them in terms of the protections they offer, run-time performance, required hardware features, etc. based on the insights, we develop a set of recommendations for the research community and mcu software developers.",2024-01-27
"86","moving beyond deletions: program simplification via diverse program transformations","haibo wang, zezhong xing, zheng wang, chengnian sun, shin hwei tan","software engineering","to reduce the complexity of software, developers manually simplify program (known as developer-induced program simplification in this paper) to reduce its code size yet preserving its functionality but manual simplification is time-consuming and error-prone. to reduce manual effort, rule-based approaches (e.g., refactoring) and deletion-based approaches (e.g., delta debugging) can be potentially applied to automate developer-induced program simplification. however, as there is little study on how developers simplify programs in open-source software (oss) projects, it is unclear whether these approaches can be effectively used for developer-induced program simplification. hence, we present the first study of developer-induced program simplification in oss projects, focusing on the types of program transformations used, the motivations behind simplifications, and the set of program transformations covered by existing refactoring types. our study of 382 pull requests from 296 projects reveals that there exist gaps in applying existing approaches for automating developer-induced program simplification. and outlines the criteria for designing automatic program simplification techniques. inspired by our study and to reduce the manual effort in developer-induced program simplification, we propose simpt5, a tool that can automatically produce simplified programs (semantically-equivalent programs with reduced source lines of code). simpt5 is trained based on our collected dataset of 92,485 simplified programs with two heuristics: (1) simplified line localization that encodes lines changed in simplified programs, and (2)checkers that measure the quality of generated programs. our evaluation shows that simpt5 are more effective than prior approaches in automating developer-induced program simplification.",2024-01-26
"87","analyzing the concept of technical debt in the context of agile software development: a systematic literature review","woubshet nema behutiye, pilar rodriguez, markku oivo, ayse tosun","software engineering","technical debt (td) is a metaphor that is used to communicate the consequences of poor software development practices to non-technical stakeholders. in recent years, it has gained significant attention in agile software development (asd). the purpose of this study is to analyze and synthesize the state of the art of td, and its causes, consequences, and management strategies in the context of asd. using a systematic literature review (slr), 38 primary studies, out of 346 studies, were identified and analyzed. we found five research areas of interest related to the literature of td in asd. among those areas, managing td in asd received the highest attention, followed by architecture in asd and its relationship with td. in addition, eight categories regarding the causes and five categories regarding the consequences of incurring td in asd were identified. focus on quick delivery and architectural and design issues were the most popular causes of incurring td in asd. reduced productivity, system degradation and increased maintenance cost were identified as significant consequences of incurring td in asd. additionally, we found 12 strategies for managing td in the context of asd, out of which refactoring and enhancing the visibility of td were the most significant. the results of this study provide a structured synthesis of td and its management in the context of asd as well as potential research areas for further investigation.",2024-01-26
"88","a first look at the general data protection regulation (gdpr) in open-source software","lucas franke, huayu liang, aaron brantly, james c davis, chris brown","software engineering","this poster describes work on the general data protection regulation (gdpr) in open-source software. although open-source software is commonly integrated into regulated software, and thus must be engineered or adapted for compliance, we do not know how such laws impact open-source software development.
we surveyed open-source developers (n=47) to understand their experiences and perceptions of gdpr. we learned many engineering challenges, primarily regarding the management of users' data and assessments of compliance. we call for improved policy-related resources, especially tools to support data privacy regulation implementation and compliance in open-source software.",2024-01-26
"89","zs4c: zero-shot synthesis of compilable code for incomplete code snippets using chatgpt","azmain kabir, shaowei wang, yuan tian, tse-hsun (peter)chen, muhammad asaduzzaman, wenbin zhang","software engineering","technical question and answering (q&a) sites such as stack overflow have become an important source for software developers to seek knowledge. however, code snippets on q&a sites are usually uncompilable and semantically incomplete for compilation due to unresolved types and missing dependent libraries, which raises the obstacle for users to reuse or analyze q&a code snippets. prior approaches either are not designed for synthesizing compilable code or suffer from a low compilation success rate. to address this problem, we propose zs4c, a lightweight approach to perform zero-shot synthesis of compilable code from incomplete code snippets using large language model (llm). zs4c operates in two stages. in the first stage, zs4c utilizes an llm, i.e., chatgpt, to identify missing import statements for a given code snippet, leveraging our designed task-specific prompt template. in the second stage, zs4c fixes compilation errors caused by incorrect import statements and syntax errors through collaborative work between chatgpt and a compiler. we thoroughly evaluated zs4c on a widely used benchmark called stattype-so against the sota approach snr. compared with snr, zs4c improves the compilation rate from 63% to 87.6%, with a 39.3% improvement. on average, zs4c can infer more accurate import statements than snr, with an improvement of 6.6% in the f1.",2024-01-25
"90","contract usage and evolution in android mobile applications","david r. ferreira, alexandra mendes, joão f. ferreira","software engineering","formal contracts and assertions are effective methods to enhance software quality by enforcing preconditions, postconditions, and invariants. previous research has demonstrated the value of contracts in traditional software development contexts. however, the adoption and impact of contracts in the context of mobile application development, particularly of android applications, remain unexplored.
to address this, we present the first large-scale empirical study on the presence and use of contracts in android applications, written in java or kotlin. we consider different types of contract elements divided into five categories: conditional runtime exceptions, apis, annotations, assertions, and other. we analyzed 2,390 android applications from the f-droid repository and processed more than 51,749 kloc to determine 1) how and to what extent contracts are used, 2) how contract usage evolves, and 3) whether contracts are used safely in the context of program evolution and inheritance. our findings include: 1) although most applications do not specify contracts, annotation-based approaches are the most popular among practitioners; 2) applications that use contracts continue to use them in later versions, but the number of methods increases at a higher rate than the number of contracts; and 3) there are many potentially unsafe specification changes when applications evolve and in subtyping relationships, which indicates a lack of specification stability. our findings show that it would be desirable to have libraries that standardize contract specifications in java and kotlin, and tools that aid practitioners in writing stronger contracts and in detecting contract violations in the context of program evolution and inheritance.",2024-01-25
"91","deepseek-coder: when the large language model meets programming -- the rise of code intelligence","daya guo, qihao zhu, dejian yang, zhenda xie, kai dong, wentao zhang, guanting chen, xiao bi, y. wu, y.k. li, fuli luo, yingfei xiong, wenfeng liang","software engineering","the rapid development of large language models has revolutionized code intelligence in software development. however, the predominance of closed-source models has restricted extensive research and development. to address this, we introduce the deepseek-coder series, a range of open-source code models with sizes from 1.3b to 33b, trained from scratch on 2 trillion tokens. these models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16k window to enhance code generation and infilling. our extensive evaluations demonstrate that deepseek-coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like codex and gpt-3.5. furthermore, deepseek-coder models are under a permissive license that allows for both research and unrestricted commercial use.",2024-01-25
"92","from requirements to architecture: an ai-based journey to semi-automatically generate software architectures","tobias eisenreich, sandro speth, stefan wagner","software engineering","designing domain models and software architectures represents a significant challenge in software development, as the resulting architectures play a vital role in fulfilling the system's quality of service. due to time pressure, architects often model only one architecture based on their known limited domain understanding, patterns, and experience instead of thoroughly analyzing the domain and evaluating multiple candidates, selecting the best fitting. existing approaches try to generate domain models based on requirements, but still require time-consuming manual effort to achieve good results. therefore, in this vision paper, we propose a method to generate software architecture candidates semi-automatically based on requirements using artificial intelligence techniques. we further envision an automatic evaluation and trade-off analysis of the generated architecture candidates using, e.g., the architecture trade-off analysis method combined with large language models and quantitative analyses. to evaluate this approach, we aim to analyze the quality of the generated architecture models and the efficiency and effectiveness of our proposed process by conducting qualitative studies.",2024-01-25
"93","what makes a great software quality assurance engineer?","roselane silva farias, iftekhar ahmed, eduardo santana de almeida","software engineering","software quality assurance (sqa) engineers are responsible for assessing a product during every phase of the software development process to ensure that the outcomes of each phase and the final product possess the desired qualities. in general, a great sqa engineer needs to have a different set of abilities from development engineers to effectively oversee the entire product development process from beginning to end. recent empirical studies identified important attributes of software engineers and managers, but the quality assurance role is overlooked. as software quality aspects have become more of a priority in the life cycle of software development, employers seek professionals that best suit the company's objectives and new graduates desire to make a valuable contribution through their job as an sqa engineer, but what makes them great? we addressed this knowledge gap by conducting 25 semi-structured interviews and 363 survey respondents with software quality assurance engineers from different companies around the world. we use the data collected from these activities to derive a comprehensive set of attributes that are considered important. as a result of the interviews, twenty-five attributes were identified and grouped into five main categories: personal, social, technical, management, and decision-making attributes. through a rating survey, we confirmed that the distinguishing characteristics of great sqa engineers are curiosity, the ability to communicate effectively, and critical thinking skills. this work will guide further studies with sqa practitioners, by considering contextual factors and providing some implications for research and practice.",2024-01-24
"94","can large language models write parallel code?","daniel nichols, joshua h. davis, zhaojun xie, arjun rajaram, abhinav bhatele","distributed, parallel, and cluster computing","large language models are becoming an increasingly popular tool for software development. their ability to model and generate source code has been demonstrated in a variety of contexts, including code completion, summarization, translation, and lookup. however, they often struggle to generate code for more complex tasks. in this paper, we explore the ability of state-of-the-art language models to generate parallel code. we propose a benchmark, pcgbench, consisting of a set of 420 tasks for evaluating the ability of language models to generate parallel code, and we evaluate the performance of several state-of-the-art open- and closed-source language models on these tasks. we introduce novel metrics for comparing parallel code generation performance and use them to explore how well each llm performs on various parallel programming models and computational problem types.",2024-01-23
"95","software engineering for robotics: future research directions; report from the 2023 workshop on software engineering for robotics","claire le goues (carnegie mellon university), sebastian elbaum (university of virginia), david anthony (southwest research institute), z. berkay celik (purdue university), mauricio castillo-effen (lockheed martin), nikolaus correll (university of colorado-boulder), pooyan jamshidi (university of south carolina), morgan quigley (open source robotics foundation), trenton tabor (carnegie mellon university), qi zhu (northwestern university)","robotics","robots are experiencing a revolution as they permeate many aspects of our daily lives, from performing house maintenance to infrastructure inspection, from efficiently warehousing goods to autonomous vehicles, and more. this technical progress and its impact are astounding. this revolution, however, is outstripping the capabilities of existing software development processes, techniques, and tools, which largely have remained unchanged for decades. these capabilities are ill-suited to handling the challenges unique to robotics software such as dealing with a wide diversity of domains, heterogeneous hardware, programmed and learned components, complex physical environments captured and modeled with uncertainty, emergent behaviors that include human interactions, and scalability demands that span across multiple dimensions.
looking ahead to the need to develop software for robots that are ever more ubiquitous, autonomous, and reliant on complex adaptive components, hardware, and data, motivated an nsf-sponsored community workshop on the subject of software engineering for robotics, held in detroit, michigan in october 2023. the goal of the workshop was to bring together thought leaders across robotics and software engineering to coalesce a community, and identify key problems in the area of se for robotics that that community should aim to solve over the next 5 years. this report serves to summarize the motivation, activities, and findings of that workshop, in particular by articulating the challenges unique to robot software, and identifying a vision for fruitful near-term research directions to tackle them.",2024-01-22
"96","the effect of predictive formal modelling at runtime on performance in human-swarm interaction","ayodeji o. abioye, william hunt, yue gu, eike schneiders, mohammad naiseh, joel e. fischer, sarvapali d. ramchurn, mohammad d. soorati, blair archibald, michele sevegnani","robotics","formal modelling is often used as part of the design and testing process of software development to ensure that components operate within suitable bounds even in unexpected circumstances. in this paper, we use predictive formal modelling (pfm) at runtime in a human-swarm mission and show that this integration can be used to improve the performance of human-swarm teams. we recruited 60 participants to operate a simulated aerial swarm to deliver parcels to target locations. in the pfm condition, operators were informed of the estimated completion times given the number of drones deployed, whereas in the no-pfm condition, operators did not have this information. the operators could control the mission by adding or removing drones from the mission and thereby, increasing or decreasing the overall mission cost. the evaluation of human-swarm performance relied on four key metrics: the time taken to complete tasks, the number of agents involved, the total number of tasks accomplished, and the overall cost associated with the human-swarm task. our results show that pfm modelling at runtime improves mission performance without significantly affecting the operator's workload or the system's usability.",2024-01-22
"97","revolutionizing api documentation through summarization","amirhossein naghshzan, sylvie ratte","software engineering","this study tackles the challenges associated with interpreting application programming interface (api) documentation, an integral aspect of software development. official api documentation, while essential, can be lengthy and challenging to navigate, prompting developers to seek unofficial sources such as stack overflow. leveraging the vast user-generated content on stack overflow, including code snippets and discussions, we employ bertopic and extractive summarization to automatically generate concise and informative api summaries. these summaries encompass key insights like general usage, common developer issues, and potential solutions, sourced from the wealth of knowledge on stack overflow. software developers evaluate these summaries for performance, coherence, and interoperability, providing valuable feedback on the practicality of our approach.",2024-01-21
"98","binaryai: binary software composition analysis via intelligent binary source code matching","ling jiang, junwen an, huihui huang, qiyi tang, sen nie, shi wu, yuqun zhang","software engineering","while third-party libraries are extensively reused to enhance productivity during software development, they can also introduce potential security risks such as vulnerability propagation. software composition analysis, proposed to identify reused tpls for reducing such risks, has become an essential procedure within modern devsecops. as one of the mainstream sca techniques, binary-to-source sca identifies the third-party source projects contained in binary files via binary source code matching, which is a major challenge in reverse engineering since binary and source code exhibit substantial disparities after compilation. the existing binary-to-source sca techniques leverage basic syntactic features that suffer from redundancy and lack robustness in the large-scale tpl dataset, leading to inevitable false positives and compromised recall. to mitigate these limitations, we introduce binaryai, a novel binary-to-source sca technique with two-phase binary source code matching to capture both syntactic and semantic code features. first, binaryai trains a transformer-based model to produce function-level embeddings and obtain similar source functions for each binary function accordingly. then by applying the link-time locality to facilitate function matching, binaryai detects the reused tpls based on the ratio of matched source functions. our experimental results demonstrate the superior performance of binaryai in terms of binary source code matching and the downstream sca task. specifically, our embedding model outperforms the state-of-the-art model codecmr, i.e., achieving 22.54% recall@1 and 0.34 mrr compared with 10.75% and 0.17 respectively. additionally, binaryai outperforms all existing binary-to-source sca tools in tpl detection, increasing the precision from 73.36% to 85.84% and recall from 59.81% to 64.98% compared with the well-recognized commercial sca product black duck.",2024-01-20
"99","boidae: your personal mining platform","brian sigurdson, samuel w. flint, robert dyer","software engineering","mining software repositories is a useful technique for researchers and practitioners to see what software developers actually do when developing software. tools like boa provide users with the ability to easily mine these open-source software repositories at a very large scale, with datasets containing hundreds of thousands of projects. the trade-off is that users must use the provided infrastructure, query language, runtime, and datasets and this might not fit all analysis needs. in this work, we present boidae: a family of boa installations controlled and customized by users. boidae uses automation tools such as ansible and docker to facilitate the deployment of a customized boa installation. in particular, boidae allows the creation of custom datasets generated from any set of git repositories, with helper scripts to aid in finding and cloning repositories from github and sourceforge. in this paper, we briefly describe the architecture of boidae and how researchers can utilize the infrastructure to generate custom datasets. boidae's scripts and all infrastructure it builds upon are open-sourced. a video demonstration of boidae's installation and extension is available at this https url.",2024-01-20
"100","code reviewer recommendation based on a hypergraph with multiplex relationships","yu qiao, jian wang, can cheng, wei tang, peng liang, yuqi zhao, bing li","software engineering","code review is an essential component of software development, playing a vital role in ensuring a comprehensive check of code changes. however, the continuous influx of pull requests and the limited pool of available reviewer candidates pose a significant challenge to the review process, making the task of assigning suitable reviewers to each review request increasingly difficult. to tackle this issue, we present mirrec, a novel code reviewer recommendation method that leverages a hypergraph with multiplex relationships. mirrec encodes high-order correlations that go beyond traditional pairwise connections using degree-free hyperedges among pull requests and developers. this way, it can capture high-order implicit connectivity and identify potential reviewers. to validate the effectiveness of mirrec, we conducted experiments using a dataset comprising 48,374 pull requests from ten popular open-source software projects hosted on github. the experiment results demonstrate that mirrec, especially without pr-review commenters relationship, outperforms existing stateof-the-art code reviewer recommendation methods in terms of acc and mrr, highlighting its significance in improving the code review process.",2024-01-19
"101","in-ide human-ai experience in the era of large language models; a literature review","agnia sergeyuk, sergey titov, maliheh izadi","software engineering","integrated development environments (ides) have become central to modern software development, especially with the integration of artificial intelligence (ai) to enhance programming efficiency and decision-making. the study of in-ide human-ai experience is critical in understanding how these ai tools are transforming the software development process, impacting programmer productivity, and influencing code quality. we conducted a literature review to study the current state of in-ide human-ai experience research, bridging a gap in understanding the nuanced interactions between programmers and ai assistants within ides. by analyzing 36 selected papers, our study illustrates three primary research branches: design, impact, and quality of interaction. the trends, challenges, and opportunities identified in this paper emphasize the evolving landscape of software development and inform future directions for research and development in this dynamic field. specifically, we invite the community to investigate three aspects of these interactions: designing task-specific user interface, building trust, and improving readability.",2024-01-19
"102","catch the butterfly: peeking into the terms and conflicts among spdx licenses","tao liu, chengwei liu, tianwei liu, he wang, gaofei wu, yang liu, yuqing zhang","software engineering","the widespread adoption of third-party libraries (tpls) in software development has accelerated the creation of modern software. however, this convenience comes with potential legal risks. developers may inadvertently violate the licenses of tpls, leading to legal issues. while existing studies have explored software licenses and potential incompatibilities, these studies often focus on a limited set of licenses or rely on low-quality license data, which may affect their conclusions. to address this gap, there is a need for a high-quality license dataset that encompasses a broad range of mainstream licenses to help developers navigate the complex landscape of software licenses, avoid potential legal pitfalls, and guide solutions for managing license compliance and compatibility in software development. to this end, we conduct the first work to understand the mainstream software licenses based on term granularity and obtain a high-quality dataset of 453 spdx licenses with well-labeled terms and conflicts. specifically, we first conduct a differential analysis of the mainstream platforms to understand the terms and attitudes of each license. next, we propose a standardized set of license terms to capture and label existing mainstream licenses with high quality. moreover, we include copyleft conflicts and conclude the three major types of license conflicts among the 453 spdx licenses. based on these, we carry out two empirical studies to reveal the concerns and threats from the perspectives of both licensors and licensees. one study provides an in-depth analysis of the similarities, differences, and conflicts among spdx licenses, revisits the usage and conflicts of licenses in the npm ecosystem, and draws conclusions that differ from previous work. our studies reveal some insightful findings and disclose relevant analytical data, which set the stage for further research.",2024-01-19
"103","enablers and barriers of empathy in software developer and user interaction: a mixed methods case study","hashini gunatilake, john grundy, rashina hoda, ingo mueller","software engineering","software engineering (se) requires developers to collaborate with stakeholders, and understanding their emotions and perspectives is often vital. empathy is a concept characterising a person's ability to understand and share the feelings of another. however, empathy continues to be an under-researched human aspect in se. we studied how empathy is practised between developers and end users using a mixed methods case study. we used an empathy test, observations and interviews to collect data, and socio technical grounded theory and descriptive statistics to analyse data. we identified the nature of awareness required to trigger empathy and enablers of empathy. we discovered barriers to empathy and a set of potential strategies to overcome these barriers. we report insights on emerging relationships and present a set of recommendations and potential future works on empathy and se for software practitioners and se researchers.",2024-01-17
"104","specgen: automated generation of formal program specifications via large language models","lezhi ma, shangqing liu, yi li, xiaofei xie, lei bu","software engineering","in software development, formal program specifications play a crucial role in various stages. however, manually crafting formal program specifications is rather difficult, making the job time-consuming and labor-intensive. moreover, it is even more challenging to write specifications that correctly and comprehensively describe the semantics of complex programs. to reduce the burden on software developers, automated specification generation methods have emerged. however, existing methods usually rely on predefined templates or grammar, making them struggle to accurately describe the behavior and functionality of complex real-world programs. to tackle this challenge, we introduce specgen, a novel technique for formal program specification generation based on large language models. our key insight is to overcome the limitations of existing methods by leveraging the code comprehension capability of llms. the process of specgen consists of two phases. the first phase employs a conversational approach that guides the llm to generate appropriate specifications for a given program. the second phase, designed for where the llm fails to generate correct specifications, applies four mutation operators to the model-generated specifications and selects verifiable specifications from the mutated ones through a novel heuristic selection strategy by assigning different weights of variants in an efficient manner. to evaluate the performance of specgen, we manually construct a dataset containing 120 test cases. our experimental results demonstrate that specgen succeeds in generating verifiable specifications for 100 out of 120 programs, outperforming the existing purely llm-based approaches and conventional specification generation tools. further investigations on the quality of generated specifications indicate that specgen can comprehensively articulate the behaviors of the input program.",2024-01-16
"105","incentivizing secure software development: the role of liability (waiver) and audit","ziyuan huang, gergely biczók, mingyan liu","cryptography and security","misaligned incentives in secure software development have long been the focus of research in the economics of security. product liability, a powerful legal framework in other industries, has been largely ineffective for software products until recent times. however, the rapid regulatory responses to recent global cyberattacks by both the united states and the european union, together with the (relative) success of the general data protection regulation in defining both duty and standard of care for software vendors, may just enable regulators to use liability to re-align incentives for the benefit of the digital society. specifically, the recently proposed united states national cybersecurity strategy shifts responsibility for cyber incidents back to software vendors. in doing so, the strategy also puts forward the concept of the liability waiver: if a software company voluntarily undergoes and passes an it security audit, its liability is waived.
in this paper, we analyze this audit scenario from the aspect of the software vendor. we propose a mechanism where a software vendor should first undergo a repeated auditing process in each stage of which the vendor decides whether to quit early or stay with additional security investment. we show that the optimal strategy for an opt-in vendor is to never quit; and exert cumulative investments in either ""one-and-done"" or ""incremental"" manner. we relate the audit mechanism to a liability waiver insurance policy and revealed its effect on reshaping the vendor's risk perception. we also discuss influence of audit quality on the vendor's incentives and pinpoint that a desirable audit rule should be highly accurate and less strict.",2024-01-16
"106","agile meets quantum: a novel genetic algorithm model for predicting the success of quantum software development project","arif ali khan, muhammad azeem akbar, valtteri lahtinen, marko paavola","software engineering","context: quantum software systems represent a new realm in software engineering, utilizing quantum bits (qubits) and quantum gates (qgates) to solve the complex problems more efficiently than classical counterparts . agile software development approaches are considered to address many inherent challenges in quantum software development, but their effective integration remains unexplored objective: this study investigates key causes of challenges that could hinders the adoption of traditional agile approaches in quantum software projects and develop an agile quantum software project success prediction model (aqsspm). methodology: firstly, w e identified 19 causes of challenging factors discussed in our previous study, which are potentially impacting agile quantum project success. secondly, a survey was conducted to collect expert opinions on these causes and applied genetic algorithm (ga) with na i ve bayes classifier (nbc) and logistic regression (lr) to develop the aqsspm results: utilizing ga with nbc, project success probability improved from 53.17% to 99.68%, with cost reductions from 0.463% to 0.403%. similarly, ga with lr increased success rates from 55.52% to 98.99%, and costs decreased from 0.496% to 0.409% after 100 iterati ons. both methods result showed a strong positive correlation (rs=0.955) in causes ranking, with no significant difference between them (t=1.195, p=0.240>0.05). conclusion: the aqsspm highlights critical focus areas for efficiently and successfully implementing agile quantum projects considering the cost factor of a particular project",2024-01-16
"107","from digital twins to digital twin prototypes: concepts, formalization, and applications","alexander barbie, wilhelm hasselbring","software engineering","the transformation to industry 4.0 also transforms the processes of how we develop intelligent manufacturing production systems. to advance the software development of these new (embedded) software systems, digital twins may be employed. however, there is no consensual definition of what a digital twin is. in this paper, we give an overview of the current state of the digital twin concept and formalize the digital twin concept using the object-z notation. this formalization includes the concepts of physical twins, digital models, digital templates, digital threads, digital shadows, digital twins, and digital twin prototypes. the relationships between all these concepts are visualized as uml class diagrams.
our digital twin prototype (dtp) approach supports engineers during the development and automated testing of complex embedded software systems. this approach enable engineers to test embedded software systems in a virtual context, without the need of a connection to a physical object. in continuous integration / continuous deployment pipelines such digital twin prototypes can be used for automated integration testing and, thus, allow for an agile verification and validation process.
in this paper, we demonstrate and report on how to apply and implement a digital twin by the example of two real-world field studies (ocean observation systems and smart farming). for independent replication and extension of our approach by other researchers, we provide a lab study published open source on github.",2024-01-15
"108","admin: attacks on dataset, model and input. a threat model for ai based software","vimal kumar, juliette mayo, khadija bahiss","cryptography and security","machine learning (ml) and artificial intelligence (ai) techniques have now become commonplace in software products and services. when threat modelling a system, it is therefore important that we consider threats unique to ml and ai techniques, in addition to threats to our software. in this paper, we present a threat model that can be used to systematically uncover threats to ai based software. the threat model consists of two main parts, a model of the software development process for ai based software and an attack taxonomy that has been developed using attacks found in adversarial ai research. we apply the threat model to two real life ai based software and discuss the process and the threats found.",2024-01-15
"109","tdd without tears: towards test case generation from requirements through deep reinforcement learning","wannita takerngsaksiri, rujikorn charakorn, chakkrit tantithamthavorn, yuan-fang li","software engineering","test-driven development (tdd) is a widely-employed software development practice that mandates writing test cases based on requirements before writing the actual code. while writing test cases is the centerpiece of tdd, it is time-consuming, expensive, and often shunned by developers. to address these issues associated with tdd, automated test case generation approaches have recently been investigated. such approaches take source code as input, but not the requirements. therefore, existing work does not fully support true tdd, as actual code is required to generate test cases. in addition, current deep learning-based test case generation approaches are trained with one learning objective, i.e., to generate test cases that are exactly matched with the ground-truth test cases. however, such approaches may limit the model's ability to generate different yet correct test cases. in this paper, we introduce pytester, a text-to-testcase generation approach that can automatically generate syntactically correct, executable, complete, and effective test cases while being aligned with a given natural language requirement. we evaluate pytester on the public apps benchmark dataset, and the results show that our deep rl approach enables pytester, a small language model, to outperform much larger language models like gpt3.5, starcoder, and incoder. our findings suggest that future research could consider improving small over large lms for better resource efficiency by integrating the se domain knowledge into the design of reinforcement learning architecture.",2024-01-15
"110","codeagent: enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges","kechi zhang, jia li, ge li, xianjie shi, zhi jin","software engineering","large language models (llms) have shown promise in automated code generation but typically excel only in simpler tasks such as generating standalone code units. real-world software development, however, often involves complex code repositories (named repo) with complex dependencies and extensive documentation. to fill this gap, our research pivots towards evaluating llms in a more realistic setting -- real-world repo-level code generation. we introduce codeagentbench, a manually curated benchmark for repo-level code generation. this benchmark comprises five high-quality python projects, encompassing a total of 101 samples. we assess nine leading llms on repo-level tasks and observe a decline in their performance. to tackle this, we present codeagent, a novel llm-based agent framework that employs external tools for effective repo-level code generation. codeagent integrates five programming tools, enabling interaction with software artifacts for information retrieval, code symbol navigation, and code testing. we implement four agent strategies to optimize these tools' usage. our experiments on codeagentbench show that codeagent enhances llm performance significantly, with improvements ranging from 18.1\% to 250\%. further tests on the humaneval benchmark confirm codeagent's adaptability and efficacy across various code generation tasks. notably, codeagent outperforms commercial products like github copilot, showcasing superior accuracy and efficiency. these results demonstrate codeagent's robust capabilities in code generation, highlighting its potential for real-world repo-level coding challenges.",2024-01-14
"111","geml: a grammar-based evolutionary machine learning approach for design-pattern detection","rafael barbudo, aurora ramírez, francisco servant, josé raúl romero","software engineering","design patterns (dps) are recognised as a good practice in software development. however, the lack of appropriate documentation often hampers traceability, and their benefits are blurred among thousands of lines of code. automatic methods for dp detection have become relevant but are usually based on the rigid analysis of either software metrics or specific properties of the source code. we propose geml, a novel detection approach based on evolutionary machine learning using software properties of diverse nature. firstly, geml makes use of an evolutionary algorithm to extract those characteristics that better describe the dp, formulated in terms of human-readable rules, whose syntax is conformant with a context-free grammar. secondly, a rule-based classifier is built to predict whether new code contains a hidden dp implementation. geml has been validated over five dps taken from a public repository recurrently adopted by machine learning studies. then, we increase this number up to 15 diverse dps, showing its effectiveness and robustness in terms of detection capability. an initial parameter study served to tune a parameter setup whose performance guarantees the general applicability of this approach without the need to adjust complex parameters to a specific pattern. finally, a demonstration tool is also provided.",2024-01-13
"112","invisible labor in open source software ecosystems","john meluso, amanda casari, katie mclaughlin, milo z. trujillo","software engineering","invisible labor is work that is not fully visible, not appropriately compensated, or both. in open source software (oss) ecosystems, essential tasks that do not involve code (like content moderation) often become invisible to the detriment of individuals and organizations. however, invisible labor is so difficult to measure that we do not know how much of oss activities are invisible. our study addresses this challenge, demonstrating that roughly half of oss work is invisible. we do this by developing a survey technique with cognitive anchoring that measures oss developer self-assessments of labor visibility and attribution. survey respondents (n=142) reported that their work is more likely to be nonvisible or partially visible (i.e. visible to at most 1 other person) than fully visible (i.e. visible to 2 or more people). furthermore, cognitively anchoring participants to the idea of high work visibility increased perceptions of labor visibility and decreased visibility importance compared to anchoring to low work visibility. this suggests that advertising oss activities as ""open"" may not make labor visible to most people, but rather lead contributors to overestimate labor visibility. we therefore add to a growing body of evidence that designing systems that recognize all kinds of labor as legitimate contributions is likely to improve fairness in software development while providing greater transparency into work designs that help organizations and communities achieve their goals.",2024-01-12
"113","automated test case repair using language models","ahmadreza saboor yaraghi, darren holden, nafiseh kahani, lionel briand","software engineering","ensuring the quality of software systems through testing is essential, yet maintaining test cases poses significant challenges and costs. the need for frequent updates to align with the evolving system under test often entails high complexity and cost for maintaining these test cases. further, unrepaired broken test cases can degrade test suite quality and disrupt the software development process, wasting developers' time. to address this challenge, we present target (test repair generator), a novel approach leveraging pre-trained code language models for automated test case repair. target treats test repair as a language translation task, employing a two-step process to fine-tune a language model based on essential context data characterizing the test breakage. to evaluate our approach, we introduce tarbench, a comprehensive benchmark we developed covering 45,373 broken test repairs across 59 open-source projects. our results demonstrate target's effectiveness, achieving a 66.1% exact match accuracy. furthermore, our study examines the effectiveness of target across different test repair scenarios. we provide a practical guide to predict situations where the generated test repairs might be less reliable. we also explore whether project-specific data is always necessary for fine-tuning and if our approach can be effective on new projects.",2024-01-12
"114","evolutionary generative fuzzing for differential testing of the kotlin compiler","calin georgescu, mitchell olsthoorn, pouria derakhshanfar, marat akhin, annibale panichella","software engineering","compiler correctness is a cornerstone of reliable software development. however, systematic testing of compilers is infeasible, given the vast space of possible programs and the complexity of modern programming languages. in this context, differential testing offers a practical methodology as it addresses the oracle problem by comparing the output of alternative compilers given the same set of programs as input. in this paper, we investigate the effectiveness of differential testing in finding bugs within the kotlin compilers developed at jetbrains. we propose a black-box generative approach that creates input programs for the k1 and k2 compilers. first, we build workable models of kotlin semantic (semantic interface) and syntactic (enriched context-free grammar) language features, which are subsequently exploited to generate random code snippets. second, we extend random sampling by introducing two genetic algorithms (gas) that aim to generate more diverse input programs. our case study shows that the proposed approach effectively detects bugs in k1 and k2; these bugs have been confirmed and (some) fixed by jetbrains developers. while we do not observe a significant difference w.r.t. the number of defects uncovered by the different search algorithms, random search and gas are complementary as they find different categories of bugs. finally, we provide insights into the relationships between the size, complexity, and fault detection capability of the generated input programs.",2024-01-12
"115","automated security findings management: a case study in industrial devops","markus voggenreiter, florian angermeir, fabiola moyón, ulrich schöpp, pierre bonvin","software engineering","in recent years, devops, the unification of development and operation workflows, has become a trend for the industrial software development lifecycle. security activities turned into an essential field of application for devops principles as they are a fundamental part of secure software development in the industry. a common practice arising from this trend is the automation of security tests that analyze a software product from several perspectives. to effectively improve the security of the analyzed product, the identified security findings must be managed and looped back to the project team for stakeholders to take action. this management must cope with several challenges ranging from low data quality to a consistent prioritization of findings while following devops aims. to manage security findings with the same efficiency as other activities in devops projects, a methodology for the management of industrial security findings minding devops principles is essential.
in this paper, we propose a methodology for the management of security findings in industrial devops projects, summarizing our research in this domain and presenting the resulting artifact. as an instance of the methodology, we developed the security flama, a semantic knowledge base for the automated management of security findings. to analyze the impact of our methodology on industrial practice, we performed a case study on two devops projects of a multinational industrial enterprise. the results emphasize the importance of using such an automated methodology in industrial devops projects, confirm our approach's usefulness and positive impact on the studied projects, and identify the communication strategy as a crucial factor for usability in practice.",2024-01-12
"116","industrial challenges in secure continuous development","fabiola moyón, florian angermeir, daniel mendez","software engineering","the intersection between security and continuous software engineering has been of great interest since the early years of the agile development movement, and it remains relevant as software development processes are more frequently guided by agility and the adoption of devops. several authors have contributed studies about the framing of secure agile development and secure devops, motivating academic contributions to methods and practices, but also discussions around benefits and challenges. especially the challenges captured also our interest since, for the last few years, we are conducting research on secure continuous software engineering from a more applied, practical perspective with the overarching aim to introduce solutions that can be adopted at scale. the short positioning at hands summarizes a relevant part of our endeavors in which we validated challenges with several practitioners of different roles. more than framing a set of challenges, we conclude by presenting four key research directions we identified for practitioners and researchers to delineate future work.",2024-01-12
"117","how dataflow diagrams impact software security analysis: an empirical experiment","simon schneider, nicolás e. díaz ferreyra, pierre-jean quéval, georg simhandl, uwe zdun, riccardo scandariato","software engineering","models of software systems are used throughout the software development lifecycle. dataflow diagrams (dfds), in particular, are well-established resources for security analysis. many techniques, such as threat modelling, are based on dfds of the analysed application. however, their impact on the performance of analysts in a security analysis setting has not been explored before. in this paper, we present the findings of an empirical experiment conducted to investigate this effect. following a within-groups design, participants were asked to solve security-relevant tasks for a given microservice application. in the control condition, the participants had to examine the source code manually. in the model-supported condition, they were additionally provided a dfd of the analysed application and traceability information linking model items to artefacts in source code. we found that the participants (n = 24) performed significantly better in answering the analysis tasks correctly in the model-supported condition (41% increase in analysis correctness). further, participants who reported using the provided traceability information performed better in giving evidence for their answers (315% increase in correctness of evidence). finally, we identified three open challenges of using dfds for security analysis based on the insights gained in the experiment.",2024-01-09
"118","what is an app store? the software engineering perspective","wenhan zhu, sebastian proksch, daniel m. german, michael w. godfrey, li li, shane mcintosh","software engineering","""app stores"" are online software stores where end users may browse, purchase, download, and install software applications. by far, the best known app stores are associated with mobile platforms, such as google play for android and apple's app store for ios. the ubiquity of smartphones has led to mobile app stores becoming a touchstone experience of modern living. however, most of app store research has concentrated on properties of the apps rather than the stores themselves. today, there is a rich diversity of app stores and these stores have largely been overlooked by researchers: app stores exist on many distinctive platforms, are aimed at different classes of users, and have different end-goals beyond simply selling a standalone app to a smartphone user.
we survey and characterize the broader dimensionality of app stores, and explore how and why they influence software development practices, such as system design and release management. we begin by collecting a set of app store examples from web search queries. by analyzing and curating the results, we derive a set of features common to app stores. we then build a dimensional model of app stores based on these features, and we fit each app store from our web search result set into this model. next, we performed unsupervised clustering to the app stores to find their natural groupings. our results suggest that app stores have become an essential stakeholder in modern software development. they control the distribution channel to end users and ensure that the applications are of suitable quality; in turn, this leads to developers adhering to various store guidelines when creating their applications. however, we found the app stores operational model could vary widely between stores, and this variability could in turn affect the generalizability of existing understanding of app stores.",2024-01-08
"119","racefixer -- an automated data race fixer","sanjay malakar, tameem bin haider, rifat shahriar","software engineering","fixing software bugs has always been an essential and time-consuming process in software development. fixing concurrency bugs has become especially critical in the multicore era. however, fixing concurrency bugs is challenging due to non-deterministic failures and tricky parallel reasoning. beyond correctly fixing the original problem in the software, a good patch should also avoid introducing new bugs, degrading performance unnecessarily, or damaging software readability. existing tools cannot automate the whole fixing process and provide good-quality patches. we present racefixer, a tool that automates the process of fixing one common type of concurrency bug: single-variable atomicity violations. racefixer starts from the bug reports of an existing bug-detection tool threadsanitizer. it augments these with static analysis to construct a suitable patch for each bug report. it tries to combine the patches of multiple bugs for better performance and code readability. finally, we test racefixer on benchmarks from theadsanitizer.",2024-01-08
"120","llm-powered code vulnerability repair with reinforcement learning and semantic reward","nafis tanveer islam, joseph khoury, andrew seong, mohammad bahrami karkevandi, gonzalo de la torre parra, elias bou-harb, peyman najafirad","software engineering","in software development, the predominant emphasis on functionality often supersedes security concerns, a trend gaining momentum with ai-driven automation tools like github copilot. these tools significantly improve developers' efficiency in functional code development. nevertheless, it remains a notable concern that such tools are also responsible for creating insecure code, predominantly because of pre-training on publicly available repositories with vulnerable code. moreover, developers are called the ""weakest link in the chain"" since they have very minimal knowledge of code security. although existing solutions provide a reasonable solution to vulnerable code, they must adequately describe and educate the developers on code security to ensure that the security issues are not repeated. therefore we introduce a multipurpose code vulnerability analysis system \texttt{secrepair}, powered by a large language model, codegen2 assisting the developer in identifying and generating fixed code along with a complete description of the vulnerability with a code comment. our innovative methodology uses a reinforcement learning paradigm to generate code comments augmented by a semantic reward mechanism. inspired by how humans fix code issues, we propose an instruction-based dataset suitable for vulnerability analysis with llms. we further identify zero-day and n-day vulnerabilities in 6 open source iot operating systems on github. our findings underscore that incorporating reinforcement learning coupled with semantic reward augments our model's performance, thereby fortifying its capacity to address code vulnerabilities with improved efficacy.",2024-01-07
"121","overwhelmed software developers: an interpretative phenomenological analysis","lisa-marie michels, aleksandra petkova, marcel richter, andreas farley, daniel graziotin, stefan wagner","software engineering","in this paper, we report on an interpretive phenomenological analysis (ipa) study on experiencing overwhelm in a software development context. the objectives of our study are, hence, to understand the experiences developers have when being overwhelmed, how this impacts their productivity and which role stress plays in the process. to this end, we interviewed two software developers who have experienced overwhelm recently. throughout a qualitative analysis of the shared experiences, we uncover seven categories of overwhelm (communication, disturbance, organizational, variety, technical, temporal, and positive overwhelm). while the first six themes all are related to negative outcomes, including low productivity and stress, the participants reported that overwhelm can sometimes be experienced to be positive and pleasant, and it can increase their mental focus, self ambition, and productivity. stress was the most mentioned feeling experienced when overwhelmed. our findings, for the most, are along the same direction of similar studies from other disciplines and with other participants. however, there may be unique attributes to software developers that mitigate the negative experiences of overwhelm.",2024-01-05
"122","""my github sponsors profile is live!"" investigating the impact of twitter/x mentions on github sponsors","youmei fan, tao xiao, hideaki hata, christoph treude, kenichi matsumoto","software engineering","github sponsors was launched in 2019, enabling donations to open-source software developers to provide financial support, as per github's slogan: ""invest in the projects you depend on"". however, a 2022 study on github sponsors found that only two-fifths of developers who were seeking sponsorship received a donation. the study found that, other than internal actions (such as offering perks to sponsors), developers had advertised their github sponsors profiles on social media, such as twitter (also known as x). therefore, in this work, we investigate the impact of tweets that contain links to github sponsors profiles on sponsorship, as well as their reception on twitter/x. we further characterize these tweets to understand their context and find that (1) such tweets have the impact of increasing the number of sponsors acquired, (2) compared to other donation platforms such as open collective and patreon, github sponsors has significantly fewer interactions but is more visible on twitter/x, and (3) developers tend to contribute more to open-source software during the week of posting such tweets. our findings are the first step toward investigating the impact of social media on obtaining funding to sustain open-source software.",2024-01-05
"123","the vulnerability is in the details: locating fine-grained information of vulnerable code identified by graph-based detectors","baijun cheng, kailong wang, cuiyun gao, xiapu luo, yulei sui, li li, yao guo, xiangqun chen, haoyu wang","software engineering","vulnerability detection is a crucial component in the software development lifecycle. existing vulnerability detectors, especially those based on deep learning (dl) models, have achieved high effectiveness. despite their capability of detecting vulnerable code snippets from given code fragments, the detectors are typically unable to further locate the fine-grained information pertaining to the vulnerability, such as the precise vulnerability triggering this http url this paper, we propose vulexplainer, a tool for automatically locating vulnerability-critical code lines from coarse-level vulnerable code snippets reported by dl-based detectors.our approach takes advantage of the code structure and the semantics of the vulnerabilities. specifically, we leverage program slicing to get a set of critical program paths containing vulnerability-triggering and vulnerability-dependent statements and rank them to pinpoint the most important one (i.e., sub-graph) as the data flow associated with the vulnerability. we demonstrate that vulexplainer performs consistently well on four state-of-the-art graph-representation(gp)-based vulnerability detectors, i.e., it can flag the vulnerability-triggering code statements with an accuracy of around 90% against eight common c/c++ vulnerabilities, outperforming five widely used gnn-based explanation approaches. the experimental results demonstrate the effectiveness of vulexplainer, which provides insights into a promising research line: integrating program slicing and deep learning for the interpretation of vulnerable code fragments.",2024-01-05
"124","dem: a method for certifying deep neural network classifier outputs in aerospace","guy katz, natan levy, idan refaeli, raz yerushalmi","software engineering","software development in the aerospace domain requires adhering to strict, high-quality standards. while there exist regulatory guidelines for commercial software in this domain (e.g., arp-4754 and do-178), these do not apply to software with deep neural network (dnn) components. consequently, it is unclear how to allow aerospace systems to benefit from the deep learning revolution. our work here seeks to address this challenge with a novel, output-centric approach for dnn certification. our method employs statistical verification techniques, and has the key advantage of being able to flag specific inputs for which the dnn's output may be unreliable - so that they may be later inspected by a human expert. to achieve this, our method conducts a statistical analysis of the dnn's predictions for other, nearby inputs, in order to detect inconsistencies. this is in contrast to existing techniques, which typically attempt to certify the entire dnn, as opposed to individual outputs. our method uses the dnn as a black-box, and makes no assumptions about its topology. we hope that this work constitutes another step towards integrating dnns in safety-critical applications - especially in the aerospace domain, where high standards of quality and reliability are crucial.",2024-01-04
"125","unit testing in asp revisited: language and test-driven development environment","giovanni amendola, tobias berei, giuseppe mazzotta, francesco ricca","software engineering","unit testing frameworks are nowadays considered a best practice, included in almost all modern software development processes, to achieve rapid development of correct specifications. knowledge representation and reasoning paradigms such as answer set programming (asp), that have been used in industry-level applications, are not an exception. indeed, the first unit testing specification language for asp was proposed in 2011 as a feature of the aspide development environment. later, a more portable unit testing language was included in the lana annotation language. in this paper we revisit both languages and tools for unit testing in asp. we propose a new unit test specification language that allows one to inline tests within asp programs, and we identify the computational complexity of the tasks associated with checking the various program-correctness assertions. test-case specifications are transparent to the traditional evaluation, but can be interpreted by a specific testing tool. thus, we present a novel environment supporting test driven development of asp programs.",2024-01-04
"126","moduleguard:understanding and detecting module conflicts in python ecosystem","ruofan zhu, xingyu wang, chengwei liu, zhengzi xu, wenbo shen, rui chang, yang liu","software engineering","python has become one of the most popular programming languages for software development due to its simplicity, readability, and versatility. as the python ecosystem grows, developers face increasing challenges in avoiding module conflicts, which occur when different packages have the same namespace modules. unfortunately, existing work has neither investigated the module conflict comprehensively nor provided tools to detect the conflict. therefore, this paper systematically investigates the module conflict problem and its impact on the python ecosystem. we propose a novel technique called instsimulator, which leverages semantics and installation simulation to achieve accurate and efficient module extraction. based on this, we implement a tool called moduleguard to detect module conflicts for the python ecosystem. for the study, we first collect 97 mc issues, classify the characteristics and causes of these mc issues, summarize three different conflict patterns, and analyze their potential threats. then, we conducted a large-scale analysis of the whole pypi ecosystem (4.2 million packages) and github popular projects (3,711 projects) to detect each mc pattern and analyze their potential impact. we discovered that module conflicts still impact numerous tpls and github projects. this is primarily due to developers' lack of understanding of the modules within their direct dependencies, not to mention the modules of the transitive dependencies. our work reveals python's shortcomings in handling naming conflicts and provides a tool and guidelines for developers to detect conflicts.",2024-01-04
"127","using ai/ml to find and remediate enterprise secrets in code & document sharing platforms","gregor kerr, david algorry, senad ibraimoski, peter maciver, sean moran","software engineering","we introduce a new challenge to the software development community: 1) leveraging ai to accurately detect and flag up secrets in code and on popular document sharing platforms that frequently used by developers, such as confluence and 2) automatically remediating the detections (e.g. by suggesting password vault functionality). this is a challenging, and mostly unaddressed task. existing methods leverage heuristics and regular expressions, that can be very noisy, and therefore increase toil on developers. the next step - modifying code itself - to automatically remediate a detection, is a complex task. we introduce two baseline ai models that have good detection performance and propose an automatic mechanism for remediating secrets found in code, opening up the study of this task to the wider community.",2024-01-03
"128","codefuse-query: a data-centric static code analysis system for large-scale organizations","xiaoheng xie, gang fan, xiaojun lin, ang zhou, shijie li, xunjin zheng, yinan liang, yu zhang, na yu, haokun li, xinyu chen, yingzhuang chen, yi zhen, dejun dong, xianjin fu, jinzhou su, fuxiong pan, pengshuai luo, youzheng feng, ruoxiang hu, jing fan, jinguo zhou, xiao xiao, peng di","software engineering","in the domain of large-scale software development, the demands for dynamic and multifaceted static code analysis exceed the capabilities of traditional tools. to bridge this gap, we present codefuse-query, a system that redefines static code analysis through the fusion of domain optimized system design and logic oriented computation design.
codefuse-query reimagines code analysis as a data computation task, support scanning over 10 billion lines of code daily and more than 300 different tasks. it optimizes resource utilization, prioritizes data reusability, applies incremental code extraction, and introduces tasks types specially for code change, underscoring its domain-optimized design. the system's logic-oriented facet employs datalog, utilizing a unique two-tiered schema, coref, to convert source code into data facts. through godel, a distinctive language, codefuse-query enables formulation of complex tasks as logical expressions, harnessing datalog's declarative prowess.
this paper provides empirical evidence of codefuse-query's transformative approach, demonstrating its robustness, scalability, and efficiency. we also highlight its real-world impact and diverse applications, emphasizing its potential to reshape the landscape of static code analysis in the context of large-scale software development.furthermore, in the spirit of collaboration and advancing the field, our project is open-sourced and the repository is available for public access",2024-01-03
"129","experimenting a new programming practice with llms","simiao zhang, jiaping wang, guoliang dong, jun sun, yueling zhang, geguang pu","software engineering","the recent development on large language models makes automatically constructing small programs possible. it thus has the potential to free software engineers from low-level coding and allow us to focus on the perhaps more interesting parts of software development, such as requirement engineering and system testing. in this project, we develop a prototype named aisd (ai-aided software development), which is capable of taking high-level (potentially vague) user requirements as inputs, generates detailed use cases, prototype system designs, and subsequently system implementation. different from existing attempts, aisd is designed to keep the user in the loop, i.e., by repeatedly taking user feedback on use cases, high-level system designs, and prototype implementations through system testing. aisd has been evaluated with a novel benchmark of non-trivial software projects. the experimental results suggest that it might be possible to imagine a future where software engineering is reduced to requirement engineering and system testing only.",2024-01-02
"130","sok: demystifying privacy enhancing technologies through the lens of software developers","maisha boteju, thilina ranbaduge, dinusha vatsalan, nalin asanka gamagedara arachchilage","software engineering","in the absence of data protection measures, software applications lead to privacy breaches, posing threats to end-users and software organisations. privacy enhancing technologies (pets) are technical measures that protect personal data, thus minimising such privacy breaches. however, for software applications to deliver data protection using pets, software developers should actively and correctly incorporate pets into the software they develop. therefore, to uncover ways to encourage and support developers to embed pets into software, this systematic literature review (slr) analyses 39 empirical studies on developers' privacy practices. it reports the usage of six pets in software application scenarios. then, it discusses challenges developers face when integrating pets into software, ranging from intrinsic challenges, such as the unawareness of pets, to extrinsic challenges, such as the increased development cost. next, the slr presents the existing solutions to address these challenges, along with the limitations of the solutions. further, it outlines future research avenues to better understand pets from a developer perspective and minimise the challenges developers face when incorporating pets into software.",2023-12-30
"131","exploring the need of accessibility education in the software industry: insights from a survey of software professionals in india","parthasarathy p d, swaroop joshi","software engineering","a userway study in 2021 indicates that an annual global e-commerce revenue loss of approximately $16 billion can be attributed to inaccessible websites and applications. according to the 2023 webaim study, only 3.7% of the world's top one million website homepages are fully accessible. this shows that many software developers use poor coding practices that don't adhere to the web content accessibility guidelines (wcag). this research centers on software professionals and their role in addressing accessibility. this work seeks to understand (a) who within the software development community actively practices accessibility, (b) when and how accessibility is considered in the software development lifecycle, (c) the various challenges encountered in building accessible software, and (d) the resources required by software professionals to enhance product accessibility. our survey of 269 software professionals from india sheds light on the pressing need for accessibility education within the software industry. a substantial majority (69.9%, n=269) of respondents express the need for training materials, workshops, and bootcamps to enhance their accessibility skills. we present a list of actionable recommendations that can be implemented within the industry to promote accessibility awareness and skills. we also open source our raw data for further research, encouraging continued exploration in this domain.",2023-12-31
"132","the tyranny of possibilities in the design of task-oriented llm systems: a scoping survey","dhruv dhamani, mary lou maher","software engineering","this scoping survey focuses on our current understanding of the design space for task-oriented llm systems and elaborates on definitions and relationships among the available design parameters. the paper begins by defining a minimal task-oriented llm system and exploring the design space of such systems through a thought experiment contemplating the performance of diverse llm system configurations (involving single llms, single llm-based agents, and multiple llm-based agent systems) on a complex software development task and hypothesizes the results. we discuss a pattern in our results and formulate them into three conjectures. while these conjectures may be partly based on faulty assumptions, they provide a starting point for future research. the paper then surveys a select few design parameters: covering and organizing research in llm augmentation, prompting techniques, and uncertainty estimation, and discussing their significance. the paper notes the lack of focus on computational and energy efficiency in evaluating research in these areas. our survey findings provide a basis for developing the concept of linear and non-linear contexts, which we define and use to enable an agent-centric projection of prompting techniques providing a lens through which prompting techniques can be viewed as multi-agent systems. the paper discusses the implications of this lens, for the cross-pollination of research between llm prompting and llm-based multi-agent systems; and also, for the generation of synthetic training data based on existing prompting techniques in research. in all, the scoping survey presents seven conjectures that can help guide future research efforts.",2023-12-29
"133","an introduction to adaptive software security","mehran alidoost nia","software engineering","this paper presents the adaptive software security model, an innovative approach integrating the mape-k loop and the software development life cycle (sdlc). it proactively embeds security policies throughout development, reducing vulnerabilities from different levels of software engineering. three primary contributions-mape-k integration, sdlc embedding, and analytical insights-converge to create a comprehensive approach for strengthening software systems against security threats. this research represents a paradigm shift, adapting security measures with agile software development and ensuring continuous improvement in the face of evolving threats. the model emerges as a robust solution, addressing the crucial need for adaptive software security strategies in modern software development. we analytically discuss the advantages of the proposed model.",2023-12-28
"134","improving code reviewer recommendation: accuracy, latency, workload, and bystanders","peter c. rigby, seth rogers, sadruddin saleem, parth suresh, daniel suskin, patrick riggs, chandra maddila, nachiappan nagappan","software engineering","code review ensures that a peer engineer manually examines the code before it is integrated and released into production. at meta, we develop a wide range of software at scale, from social networking to software development infrastructure, such as calendar and meeting tools to continuous integration. we are constantly improving our code review system, and in this work we describe a series of experiments that were conducted across 10's of thousands of engineers and 100's of thousands of reviews.
we build upon the recommender that has been in production since 2018, revrecv1. we found that reviewers were being assigned based on prior authorship of files. we reviewed the literature for successful features and experimented with them with revrecv2 in production. the most important feature in our new model was the familiarity of the author and reviewer, we saw an overall improvement in accuracy of 14 percentage points.
prior research has shown that reviewer workload is skewed. to balance workload, we divide the reviewer score from revrecv2 by each candidate reviewers workload. we experimented with multiple types of workload to develop revrecwl. we find that reranking candidate reviewers by workload often leads to a reviewers with lower workload being selected by authors.
the bystander effect can occur when a team of reviewers is assigned the review. we mitigate the bystander effect by randomly assigning one of the recommended reviewers. having an individual who is responsible for the review, reduces the time take for reviews by -11%.",2023-12-28
"135","experiential co-learning of software-developing agents","chen qian, yufan dang, jiahao li, wei liu, weize chen, cheng yang, zhiyuan liu, maosong sun","computation and language","recent advancements in large language models (llms) have brought significant changes to various domains, especially through llm-driven autonomous agents. these agents are now capable of collaborating seamlessly, splitting tasks and enhancing accuracy, thus minimizing the need for human involvement. however, these agents often approach a diverse range of tasks in isolation, without benefiting from past experiences. this isolation can lead to repeated mistakes and inefficient trials in task solving. to this end, this paper introduces experiential co-learning, a novel framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for mutual reasoning. this paradigm, enriched with previous experiences, equips agents to more effectively address unseen tasks.",2023-12-28
"136","easyview: bringing performance profiles into integrated development environments","qidong zhao, milind chabbi, xu liu","software engineering","dynamic program analysis (also known as profiling) is well-known for its powerful capabilities of identifying performance inefficiencies in software packages. although a large number of dynamic program analysis techniques are developed in academia and industry, very few of them are widely used by software developers in their regular software developing activities. there are three major reasons. first, the dynamic analysis tools (also known as profilers) are disjoint from the coding environments such as ides and editors; frequently switching focus between them significantly complicates the entire cycle of software development. second, mastering various tools to interpret their analysis results requires substantial efforts; even worse, many tools have their own design of graphical user interfaces (gui) for data presentation, which steepens the learning curves. third, most existing tools expose few interfaces to support user-defined analysis, which makes the tools less customizable to fulfill diverse user demands. we develop easyview, a general solution to integrate the interpretation and visualization of various profiling results in the coding environments, which bridges software developers with profilers to provide easy and intuitive dynamic analysis during the code development cycle. the novelty of easyview is three-fold. first, we develop a generic data format, which enables easyview to support mainstream profilers for different languages. second, we develop a set of customizable schemes to analyze and visualize the profiles in intuitive ways. third, we tightly integrate easyview with popular coding environments, such as microsoft visual studio code, with easy code exploration and user interaction. our evaluation shows that easyview is able to support various profilers for different languages and provide unique insights into performance inefficiencies in different domains.",2023-12-27
"137","evaluating code summarization techniques: a new metric and an empirical characterization","antonio mastropaolo, matteo ciniselli, massimiliano di penta, gabriele bavota","software engineering","several code summarization techniques have been proposed in the literature to automatically document a code snippet or a function. ideally, software developers should be involved in assessing the quality of the generated summaries. however, in most cases, researchers rely on automatic evaluation metrics such as bleu, rouge, and meteor. these metrics are all based on the same assumption: the higher the textual similarity between the generated summary and a reference summary written by developers, the higher its quality. however, there are two reasons for which this assumption falls short: (i) reference summaries, e.g., code comments collected by mining software repositories, may be of low quality or even outdated; (ii) generated summaries, while using a different wording than a reference one, could be semantically equivalent to it, thus still being suitable to document the code snippet. in this paper, we perform a thorough empirical investigation on the complementarity of different types of metrics in capturing the quality of a generated summary. also, we propose to address the limitations of existing metrics by considering a new dimension, capturing the extent to which the generated summary aligns with the semantics of the documented code snippet, independently from the reference summary. to this end, we present a new metric based on contrastive learning to capture said aspect. we empirically show that the inclusion of this novel dimension enables a more effective representation of developers' evaluations regarding the quality of automatically generated summaries.",2023-12-24
"138","detecting technical debt using natural language processing approaches -- a systematic literature review","edi sutoyo, andrea capiluppi","software engineering","context: technical debt (td) is a well-known metaphor for the long-term effects of architectural decisions in software development and the trade-off between producing high-quality, effective, and efficient code and meeting a release schedule. thus, the code degrades and needs refactoring. a lack of resources, time, knowledge, or experience on the development team might cause td in any software development project. objective: in the context of td detection, nlp has been utilized to identify the presence of td automatically and even recognize specific types of td. however, the enormous variety of feature extraction approaches and ml/dl algorithms employed in the literature often hinders researchers from trying to improve their performance. method: in light of this, this slr proposes a taxonomy of feature extraction techniques and algorithms used in technical debt detection: its objective is to compare and benchmark their performance in the examined studies. results: we selected 55 articles that passed the quality evaluation of this slr. we then investigated which feature extractions and algorithms were employed to identify td in each sdlc phase. all approaches proposed in the analyzed studies were grouped into nlp, nlp+ml, and nlp+dl. this allows us to discuss the performance in three different ways. conclusion: overall, the nlp+dl group consistently outperforms in precision and f1-score for all projects, and in all but one project for the recall metric. regarding the feature extraction techniques, the ptwe consistently achieves higher precision, recall, and f1-score for each project analyzed. furthermore, td types have been mapped, when possible, to sdlc phases: this served to determine the best-performing feature extractions and algorithms for each sdlc phase. finally, based on the slr results, we also identify implications that could be of concern to researchers and practitioners.",2023-12-19
"139","susdevops: promoting sustainability to a first principle in software engineering","istvan david","software engineering","sustainability is becoming a key property of modern software systems. while there is a substantial and growing body of knowledge on engineering sustainable software, end-to-end frameworks that situate sustainability-related activities within the software delivery lifecycle are missing. in this article, we propose the susdevops framework that promotes sustainability to a first principle within a devops context. we demonstrate the lifecycle phases and techniques of susdevops through the case of a software development startup company.",2023-12-22
"140","ros package search for robot software development: a knowledge graph-based approach","shuo wang, xinjun mao, shuo yang, menghan wu, zhang zhang","software engineering","ros (robot operating system) packages have become increasingly popular as a type of software artifact that can be effectively reused in robotic software development. indeed, finding suitable ros packages that closely match the software's functional requirements from the vast number of available packages is a nontrivial task using current search methods. the traditional search methods for ros packages often involve inputting keywords related to robotic tasks into general-purpose search engines or code hosting platforms to obtain approximate results of all potentially suitable ros packages. however, the accuracy of these search methods remains relatively low because the task-related keywords may not precisely match the functionalities offered by the ros packages. to improve the search accuracy of ros packages, this paper presents a novel semantic-based search approach that relies on the semantic-level ros package knowledge graph (rpkg) to automatically retrieve the most suitable ros packages. firstly, to construct the rpkg, we employ multi-dimensional feature extraction techniques to extract semantic concepts from the dataset of ros package text descriptions. the semantic features extracted from this process result in a substantial number of entities and relationships. subsequently, we create a robot domain-specific small corpus and further fine-tune a pre-trained language model, bert-ros, to generate embeddings that effectively represent the semantics of the extracted features. these embeddings play a crucial role in facilitating semantic-level understanding and comparisons during the ros package search process within the rpkg. secondly, we introduce a novel semantic matching-based search algorithm that incorporates the weighted similarities of multiple features from user search queries, which searches out more accurate ros packages than the traditional keyword search method.",2023-12-22
"141","exploring the intersection of generative ai and software development","filipe calegario, vanilson burégio, francisco erivaldo, daniel moraes costa andrade, kailane felix, nathalia barbosa, pedro lucas da silva lucena, césar frança","software engineering","in the ever-evolving landscape of artificial intelligence (ai), the synergy between generative ai and software engineering emerges as a transformative frontier. this whitepaper delves into the unexplored realm, elucidating how generative ai techniques can revolutionize software development. spanning from project management to support and updates, we meticulously map the demands of each development stage and unveil the potential of generative ai in addressing them. techniques such as zero-shot prompting, self-consistency, and multimodal chain-of-thought are explored, showcasing their unique capabilities in enhancing generative ai models. the significance of vector embeddings, context, plugins, tools, and code assistants is underscored, emphasizing their role in capturing semantic information and amplifying generative ai capabilities. looking ahead, this intersection promises to elevate productivity, improve code quality, and streamline the software development process. this whitepaper serves as a guide for stakeholders, urging discussions and experiments in the application of generative ai in software engineering, fostering innovation and collaboration for a qualitative leap in the efficiency and effectiveness of software development.",2023-12-21
"142","energibridge: empowering software sustainability through cross-platform energy measurement","june sallou, luís cruz, thomas durieux","software engineering","in the continually evolving realm of software engineering, the need to address software energy consumption has gained increasing prominence. however, the absence of a platform-independent tool that facilitates straightforward energy measurements remains a notable gap. this paper presents energibridge, a cross-platform measurement utility that provides support for linux, windows, and macos, as well as intel, amd, and apple arm cpu architectures. in essence, energibridge serves as a bridge between energy-conscious software engineering and the diverse software environments in which it operates. it encourages a broader community to make informed decisions, minimize energy consumption, and reduce the environmental impact of software systems.
by simplifying software energy measurements, energibridge offers a valuable resource to make green software development more lightweight, education more inclusive, and research more reproducible. through the evaluation, we highlight energibridge's ability to gather energy data across diverse platforms and hardware configurations.
energibridge is publicly available on github: this https url, and a demonstration video can be viewed at: this https url.",2023-12-21
"143","domain-specific code language models: unraveling the potential for hpc codes and tasks","tal kadosh, niranjan hasabnis, vy a. vo, nadav schneider, neva krien, mihai capota, abdul wasay, nesreen ahmed, ted willke, guy tamir, yuval pinter, timothy mattson, gal oren","programming languages","with easier access to powerful compute resources, there is a growing trend in ai for software development to develop larger language models (llms) to address a variety of programming tasks. even llms applied to tasks from the high-performance computing (hpc) domain are huge in size and demand expensive compute resources for training. this is partly because these llms for hpc tasks are obtained by finetuning existing llms that support several natural and/or programming languages. we found this design choice confusing - why do we need large lms trained on natural languages and programming languages unrelated to hpc for hpc-specific tasks?
in this line of work, we aim to question choices made by existing llms by developing smaller lms for specific domains - we call them domain-specific lms. specifically, we start off with hpc as a domain and build an hpc-specific lm, named monocoder, that is orders of magnitude smaller than existing lms but delivers similar, if not better performance, on non-hpc and hpc tasks. specifically, we pre-trained monocoder on an hpc-specific dataset (named hpcorpus) of c and c++ programs mined from github. we evaluated the performance of monocoder against conventional multi-lingual llms. results demonstrate that monocoder, although much smaller than existing lms, achieves similar results on normalized-perplexity tests and much better ones in codebleu competence for high-performance and parallel code generations. furthermore, fine-tuning the base model for the specific task of parallel code generation (openmp parallel for pragmas) demonstrates outstanding results compared to gpt, especially when local misleading semantics are removed by our novel pre-processor tokompiler, showcasing the ability of domain-specific models to assist in hpc-relevant tasks.",2023-12-20
"144","automated devops pipeline generation for code repositories using large language models","deep mehta, kartik rawool, subodh gujar, bowen xu","software engineering","automating software development processes through the orchestration of github action workflows has revolutionized the efficiency and agility of software delivery pipelines. this paper presents a detailed investigation into the use of large language models (llms) specifically, gpt 3.5 and gpt 4 to generate and evaluate github action workflows for devops tasks. our methodology involves data collection from public github repositories, prompt engineering for llm utilization, and evaluation metrics encompassing exact match scores, bleu scores, and a novel devops aware score. the research scrutinizes the proficiency of gpt 3.5 and gpt 4 in generating github workflows, while assessing the influence of various prompt elements in constructing the most efficient pipeline. results indicate substantial advancements in gpt 4, particularly in devops awareness and syntax correctness. the research introduces a github app built on probot, empowering users to automate workflow generation within github ecosystem. this study contributes insights into the evolving landscape of ai-driven automation in devops practices.",2023-12-20
"145","a novel approach for rapid development based on chatgpt and prompt engineering","youjia li, jianjun shi, zheng zhang","software engineering","code generation stands as a powerful technique in modern software development, improving development efficiency, reducing errors, and fostering standardization and consistency. recently, chatgpt has exhibited immense potential in automatic code generation. however, existing researches on code generation lack guidance for practical software development process. in this study, we utilized chatgpt to develop a web-based code generation platform consisting of key components: user interface, prompt builder and backend service. specifically, prompt builder dynamically generated comprehensive prompts to enhance model generation performance. we conducted experiments on 2 datasets, evaluating the generated code through 8 widely used metrics.the results demonstrate that (1) our prompt builder is effective, resulting in a 65.06% improvement in em, a 38.45% improvement in bleu, a 15.70% improvement in codebleu, and a 50.64% improvement in pass@1. (2) in real development scenarios, 98.5% of test cases can be validated through manual validation, highlighting the genuine assistance provided by the chatgpt-based code generation approach.",2023-12-20
"146","selecting source code generation tools based on bandit algorithms","ryoto shima, masateru tsunoda, yukasa murakami, akito monden, amjed tahir, kwabena ebo bennin, koji toda, keitaro nakasai","software engineering","background: recently, code generation tools such as chatgpt have drawn attention to their performance. generally, a prior analysis of their performance is needed to select new code-generation tools from a list of candidates. without such analysis, there is a higher risk of selecting an ineffective tool, negatively affecting software development productivity. additionally, conducting prior analysis of new code generation tools takes time and effort. aim: to use a new code generation tool without prior analysis but with low risk, we propose to evaluate the new tools during software development (i.e., online optimization). method: we apply the bandit algorithm (ba) approach to help select the best code-generation tool among candidates. developers evaluate whether the result of the tool is correct or not. when code generation and evaluation are repeated, the evaluation results are saved. we utilize the stored evaluation results to select the best tool based on the ba approach. our preliminary analysis evaluated five code generation tools with 164 code generation cases using ba. result: the ba approach selected chatgpt as the best tool as the evaluation proceeded, and during the evaluation, the average accuracy by the ba approach outperformed the second-best performing tool. our results reveal the feasibility and effectiveness of ba in assisting the selection of best-performing code generation tools.",2023-12-20
"147","a case study on test case construction with large language models: unveiling practical insights and challenges","roberto francisco de lima junior, luiz fernando paes de barros presta, lucca santos borborema, vanderson nogueira da silva, marcio leal de melo dahia, anderson carlos sousa e santos","software engineering","this paper presents a detailed case study examining the application of large language models (llms) in the construction of test cases within the context of software engineering. llms, characterized by their advanced natural language processing capabilities, are increasingly garnering attention as tools to automate and enhance various aspects of the software development life cycle. leveraging a case study methodology, we systematically explore the integration of llms in the test case construction process, aiming to shed light on their practical efficacy, challenges encountered, and implications for software quality assurance. the study encompasses the selection of a representative software application, the formulation of test case construction methodologies employing llms, and the subsequent evaluation of outcomes. through a blend of qualitative and quantitative analyses, this study assesses the impact of llms on test case comprehensiveness, accuracy, and efficiency. additionally, delves into challenges such as model interpretability and adaptation to diverse software contexts. the findings from this case study contributes with nuanced insights into the practical utility of llms in the domain of test case construction, elucidating their potential benefits and limitations. by addressing real-world scenarios and complexities, this research aims to inform software practitioners and researchers alike about the tangible implications of incorporating llms into the software testing landscape, fostering a more comprehensive understanding of their role in optimizing the software development process.",2023-12-19
"148","predicting line-level defects by capturing code contexts with hierarchical transformers","parvez mahbub, mohammad masudur rahman","software engineering","software defects consume 40% of the total budget in software development and cost the global economy billions of dollars every year. unfortunately, despite the use of many software quality assurance (sqa) practices in software development (e.g., code review, continuous integration), defects may still exist in the official release of a software product. therefore, prioritizing sqa efforts for the vulnerable areas of the codebase is essential to ensure the high quality of a software release. predicting software defects at the line level could help prioritize the sqa effort but is a highly challenging task given that only ~3% of lines of a codebase could be defective. existing works on line-level defect prediction often fall short and cannot fully leverage the line-level defect information. in this paper, we propose bugsplorer, a novel deep-learning technique for line-level defect prediction. it leverages a hierarchical structure of transformer models to represent two types of code elements: code tokens and code lines. unlike the existing techniques that are optimized for file-level defect prediction, bugsplorer is optimized for a line-level defect prediction objective. our evaluation with five performance metrics shows that bugsplorer has a promising capability of predicting defective lines with 26-72% better accuracy than that of the state-of-the-art technique. it can rank the first 20% defective lines within the top 1-3% suspicious lines. thus, bugsplorer has the potential to significantly reduce sqa costs by ranking defective lines higher.",2023-12-19
"149","bio-image informatics index biii: a unique database of image analysis tools and workflows for and by the bioimaging community","chong zhang, alban gaignard, matus kalas, florian levet, felipe delestro, joakim lindblad, natasa sladoje, laure plantard, alain latour, robert haase, gabriel martins, paula sampaio, leandro scholz, neubias taggers, sébastien tosi, kota miura, julien colombelli, perrine paul-gilloteaux","quantitative methods","bio image analysis has recently become one keystone of biological research but biologists tend to get lost in a plethora of available software and the way to adjust available tools to their own image analysis problem. we present biii, bioimage informatic index (this http url), the result of the first large community effort to bridge the communities of algorithm and software developers, bioimage analysts and biologists, under the form of a web-based knowledge database crowdsourced by these communities. software tools (> 1300), image databases for benchmarking (>20) and training materials (>70) for bio image analysis are referenced and curated following standards constructed by the community and then reaching a broader audience. software tools are organized as full protocol of analysis (workflow), specific brick (component) to construct a workflow, or software platform or library (collection). they are described using edam bio imaging, which is iteratively defined using this website. all entries are exposed following fair principles and accessible for other usage.",2023-12-18
"150","code ownership in open-source ai software security","jiawen wen, dong yuan, lei ma, huaming chen","software engineering","as open-source ai software projects become an integral component in the ai software development, it is critical to develop a novel methods to ensure and measure the security of the open-source projects for developers. code ownership, pivotal in the evolution of such projects, offers insights into developer engagement and potential vulnerabilities. in this paper, we leverage the code ownership metrics to empirically investigate the correlation with the latent vulnerabilities across five prominent open-source ai software projects. the findings from the large-scale empirical study suggest a positive relationship between high-level ownership (characterised by a limited number of minor contributors) and a decrease in vulnerabilities. furthermore, we innovatively introduce the time metrics, anchored on the project's duration, individual source code file timelines, and the count of impacted releases. these metrics adeptly categorise distinct phases of open-source ai software projects and their respective vulnerability intensities. with these novel code ownership metrics, we have implemented a python-based command-line application to aid project curators and quality assurance professionals in evaluating and benchmarking their on-site projects. we anticipate this work will embark a continuous research development for securing and measuring open-source ai project security.",2023-12-18
